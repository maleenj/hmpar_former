{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/__init__.py:1848\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m-> 1848\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_meta_registrations.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     _add_op_to_registry,\n\u001b[1;32m     11\u001b[0m     _convert_out_params,\n\u001b[1;32m     12\u001b[0m     global_decomposition_table,\n\u001b[1;32m     13\u001b[0m     meta_table,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_decomp/__init__.py:243\u001b[0m\n\u001b[1;32m    239\u001b[0m             decompositions\u001b[38;5;241m.\u001b[39mpop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# See NOTE [Core ATen Ops]\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_decomp/decompositions.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, cast, Iterable, List, Optional, Tuple, Union\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_prims/__init__.py:889\u001b[0m\n\u001b[1;32m    875\u001b[0m spherical_bessel_j0 \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspherical_bessel_j0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    877\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mspherical_bessel_j0,\n\u001b[1;32m    878\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    882\u001b[0m sqrt \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    884\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msqrt,\n\u001b[1;32m    885\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    886\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    887\u001b[0m )\n\u001b[0;32m--> 889\u001b[0m tan \u001b[38;5;241m=\u001b[39m \u001b[43m_make_elementwise_unary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m tanh \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    898\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtanh,\n\u001b[1;32m    899\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    900\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    901\u001b[0m )\n\u001b[1;32m    903\u001b[0m trunc \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrunc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    905\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtrunc,\n\u001b[1;32m    906\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    907\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    908\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_prims/__init__.py:447\u001b[0m, in \u001b[0;36m_make_elementwise_unary_prim\u001b[0;34m(name, type_promotion, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_elementwise_unary_prim\u001b[39m(\n\u001b[1;32m    441\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    442\u001b[0m ):\n\u001b[1;32m    443\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m(Tensor self) -> Tensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_prims/__init__.py:274\u001b[0m, in \u001b[0;36m_make_prim\u001b[0;34m(schema, return_type, meta, impl_aten, doc, tags)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_prim\u001b[39m(\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    262\u001b[0m     schema: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     tags: Optional[Sequence[torch\u001b[38;5;241m.\u001b[39mTag]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    268\u001b[0m ):\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Creates a primitive operation.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     \u001b[43mprim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt2_compliant_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prim_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# always run the meta function because aten implementation will\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# typically accept more inputs (e.g., it will do promotion and\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# broadcasting) which we want to reject\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         meta(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/library.py:106\u001b[0m, in \u001b[0;36mLibrary.define\u001b[0;34m(self, schema, alias_analysis, tags)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tags, torch\u001b[38;5;241m.\u001b[39mTag):\n\u001b[1;32m    105\u001b[0m     tags \u001b[38;5;241m=\u001b[39m (tags,)\n\u001b[0;32m--> 106\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias_analysis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m qualname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m schema\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_defs\u001b[38;5;241m.\u001b[39madd(qualname)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileroot='2024_03_08_TRF_Maleen/'\n",
    "filename='2024_03_08_TRF_Maleen_01'\n",
    "\n",
    "datapath='/home/maleen/rosbags/Transformers/datasets/training/'\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "df1 = pd.read_pickle(datapath + filename + '.pkl')\n",
    "\n",
    "arrays = [np.array(item) for item in df1['Skeleton_3D']]\n",
    "timestamps = [np.array(item) for item in df1['Skeleton_Timestamp']]\n",
    "\n",
    "# Convert datetime to seconds from start\n",
    "timestamps = (timestamps - timestamps[0])\n",
    "\n",
    "# Stack these arrays along a new axis to create a 3D NumPy array\n",
    "# Each \"slice\" of this 3D array represents one frame of keypoints\n",
    "skeleton_3d_frames = np.stack(arrays, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create masks for the data (1 = data present, 0 = data missing)\n",
    "# Correcting the mask values\n",
    "masks = np.where(np.isnan(skeleton_3d_frames).any(axis=2), 0, 1)  # 0 for missing, 1 for present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the differences in position and time\n",
    "position_diff = np.diff(skeleton_3d_frames, axis=0)\n",
    "time_diff = np.diff(timestamps)\n",
    "\n",
    "# Ensure that time_diff is of shape (n,1,1) so that it broadcasts correctly when dividing\n",
    "time_diff = time_diff[:, np.newaxis, np.newaxis]\n",
    "\n",
    "# Update masks to match the dimensions of position_diff and time_diff\n",
    "# We use the bitwise AND operator to ensure that both the current and previous frames are valid\n",
    "masks_pos = masks[:-1, :] & masks[1:, :]\n",
    "\n",
    "# Add an additional dimension to masks\n",
    "masks_pos = masks_pos[:,:,np.newaxis]\n",
    "\n",
    "# Now we calculate velocity, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_vel = np.where(masks_pos, position_diff / time_diff, np.nan)\n",
    "\n",
    "masks_velocity = masks_pos[:-1, :] & masks_pos[1:, :]\n",
    "\n",
    "# Calculate the differences in velocity\n",
    "velocity_diff = np.diff(skel_vel, axis=0)\n",
    "\n",
    "# Now we calculate acceleration, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_acc = np.where(masks_velocity, velocity_diff / time_diff[:-1, :, :], np.nan)  # Use time_diff with one less time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, slice skeleton_3d_frames and skel_vel to match the dimensions of skell_acc\n",
    "skel_pos= skeleton_3d_frames[2:]\n",
    "skel_vel = skel_vel[1:]\n",
    "masks = masks[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to hold the normalized data, medians, and IQRs\n",
    "norm_pos = np.empty_like(skel_pos)\n",
    "medians_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "iqrs_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "\n",
    "norm_vel= np.empty_like(skel_vel)\n",
    "medians_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "iqrs_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "\n",
    "norm_acc = np.empty_like(skel_acc)\n",
    "medians_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "iqrs_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "\n",
    "def robust_normalize_data_with_clipping(data, masks, medians_per_joint_axis, iqrs_per_joint_axis, normalized_data, clipping_percentiles=(1, 99)):\n",
    "    for joint in range(data.shape[1]):  # For each joint\n",
    "        for axis in range(data.shape[2]):  # For each axis (x, y, z)\n",
    "            joint_axis_data = data[:, joint, axis]\n",
    "            mask_for_joint = masks[:, joint]\n",
    "\n",
    "            # Select valid data based on the mask\n",
    "            valid_data = joint_axis_data[mask_for_joint == 1]\n",
    "\n",
    "            # Determine clipping thresholds based on percentiles\n",
    "            lower_threshold, upper_threshold = np.percentile(valid_data, clipping_percentiles) if valid_data.size > 0 else (np.nan, np.nan)\n",
    "\n",
    "            # Clip the data based on valid mask and thresholds\n",
    "            clipped_values = np.clip(joint_axis_data, lower_threshold, upper_threshold)\n",
    "\n",
    "            # Calculate median and IQR for clipped data\n",
    "            median = np.median(clipped_values[mask_for_joint == 1]) if np.any(mask_for_joint == 1) else np.nan\n",
    "            q75, q25 = np.percentile(clipped_values[mask_for_joint == 1], [75 ,25]) if np.any(mask_for_joint == 1) else (np.nan, np.nan)\n",
    "            iqr = q75 - q25\n",
    "\n",
    "            # Store the calculated medians and IQRs\n",
    "            medians_per_joint_axis[joint, axis] = median\n",
    "            iqrs_per_joint_axis[joint, axis] = iqr\n",
    "\n",
    "            # Normalize the clipped data, avoiding division by zero\n",
    "            if iqr > 0:\n",
    "                normalized_values = (clipped_values - median) / iqr\n",
    "            else:\n",
    "                normalized_values = clipped_values  # Keep original values if IQR is 0 or nan\n",
    "\n",
    "            # Apply normalization only where data is present\n",
    "            normalized_data[:, joint, axis] = np.where(mask_for_joint == 1, normalized_values, np.nan)\n",
    "            \n",
    "    return normalized_data, medians_per_joint_axis, iqrs_per_joint_axis\n",
    "\n",
    "# Example usage with your data\n",
    "# Note: masks, skel_pos, skel_vel, skel_acc should be defined in your context\n",
    "\n",
    "norm_pos, medians_pos, iqrs_pos = robust_normalize_data_with_clipping(skel_pos, masks, medians_per_joint_axis_skel_pos, iqrs_per_joint_axis_skel_pos, norm_pos)\n",
    "norm_vel, medians_vel, iqrs_vel = robust_normalize_data_with_clipping(skel_vel, masks, medians_per_joint_axis_vel, iqrs_per_joint_axis_vel, norm_vel)\n",
    "norm_acc, medians_acc, iqrs_acc = robust_normalize_data_with_clipping(skel_acc, masks, medians_per_joint_axis_acc, iqrs_per_joint_axis_acc, norm_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=skel_acc\n",
    "norm_data=norm_acc\n",
    "# Plot the original and normalized data for a specific joint and axis\n",
    "joint, axis = 0, 0  # Change as needed\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[:, joint, axis], bins=20, alpha=0.7, label='Original')\n",
    "plt.title(\"Original Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(norm_data[:, joint, axis], bins=20, alpha=0.7, label='Normalized')\n",
    "plt.title(\"Normalized Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check the median and range of the normalized data\n",
    "normalized_median = np.nanmedian(norm_data[:, joint, axis])\n",
    "print(\"Median of normalized data:\", normalized_median)\n",
    "\n",
    "within_iqr = ((norm_data[:, joint, axis] > -2) & (norm_data[:, joint, axis] < 2)).sum()\n",
    "print(f\"Data points within [-1, 1] (IQR): {within_iqr} out of {norm_data.shape[0]}\")\n",
    "\n",
    "within_iqr2 = ((data[:, joint, axis] > -1) & (data[:, joint, axis] < 1)).sum()\n",
    "print(f\"Data points within [-1, 1] (IQR): {within_iqr2} out of {norm_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SkeletalInputEmbedding(nn.Module):\n",
    "    def __init__(self, num_joints=18, dof=3, embed_dim=128, seq_len=60):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.dof = dof\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.joint_embed = nn.Linear(dof, embed_dim)\n",
    "        self.vel_embed = nn.Linear(dof, embed_dim)\n",
    "        self.acc_embed = nn.Linear(dof, embed_dim)\n",
    "\n",
    "        self.register_buffer('positional_encoding', self.get_sinusoidal_encoding(seq_len * num_joints, embed_dim))\n",
    "\n",
    "    def forward(self, joint_positions, velocities, accelerations, mask=None):\n",
    "        # Replace NaNs in the input data\n",
    "        joint_positions = torch.nan_to_num(joint_positions)\n",
    "        velocities = torch.nan_to_num(velocities)\n",
    "        accelerations = torch.nan_to_num(accelerations)\n",
    "\n",
    "        # Apply mask after NaN replacement\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)  # Add a dimension for the features\n",
    "            \n",
    "        # Embedding and combining the embeddings\n",
    "        joint_embeddings = self.joint_embed(joint_positions)\n",
    "        vel_embeddings = self.vel_embed(velocities)\n",
    "        acc_embeddings = self.acc_embed(accelerations)\n",
    "\n",
    "        combined_embeddings = joint_embeddings + vel_embeddings + acc_embeddings\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            combined_embeddings = combined_embeddings * mask\n",
    "\n",
    "        pos_encodings = self.positional_encoding[:self.seq_len * self.num_joints, :].view(self.seq_len, self.num_joints, self.embed_dim)\n",
    "        #print(pos_encodings)\n",
    "        combined_embeddings += pos_encodings.unsqueeze(0)  # Unsqueeze to add batch dimension for broadcasting\n",
    "\n",
    "\n",
    "        combined_embeddings = combined_embeddings.view(-1, self.seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        return combined_embeddings\n",
    "\n",
    "    def get_sinusoidal_encoding(self, seq_len, embed_dim):\n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the module\n",
    "# embed_module = SkeletalInputEmbedding(num_joints=18, dof=3, embed_dim=128, seq_len=60)\n",
    "\n",
    "# # Example setup for testing the module\n",
    "# batch_size = 1\n",
    "# seq_len = 2  # Number of frames\n",
    "# num_joints = 3\n",
    "# dof = 3\n",
    "\n",
    "# # Initialize module\n",
    "# embedding_module = SkeletalInputEmbedding(num_joints=num_joints, dof=dof, embed_dim=6, seq_len=seq_len)\n",
    "\n",
    "# # Generate sample data\n",
    "# joint_positions = torch.randn(batch_size, seq_len, num_joints, dof)\n",
    "# velocities = torch.randn(batch_size, seq_len, num_joints, dof)\n",
    "# accelerations = torch.randn(batch_size, seq_len, num_joints, dof)\n",
    "# mask = torch.randint(0, 2, (batch_size, seq_len, num_joints))\n",
    "\n",
    "# print(mask)\n",
    "# # Test the forward pass\n",
    "# output_embeddings = embedding_module(joint_positions, velocities, accelerations, mask)\n",
    "\n",
    "# # Print output shapes\n",
    "# print(\"Output shape:\", output_embeddings.shape)\n",
    "\n",
    "# # Output shape and example output\n",
    "# print(\"Output shape:\", output_embeddings.shape)\n",
    "# print(\"Sample output embeddings:\", output_embeddings[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize your embedding layer\n",
    "# embedding_layer = SkeletalInputEmbedding()\n",
    "\n",
    "# # Create a dummy input tensor for joint positions, velocities, and accelerations\n",
    "# # Shape: (batch_size, seq_len, num_joints, dof)\n",
    "# joint_positions = torch.randn(1, 1, 18, 3)\n",
    "# velocities = torch.randn(1, 1, 18, 3)\n",
    "# accelerations = torch.randn(1, 1, 18, 3)\n",
    "\n",
    "# # Create a mask tensor\n",
    "# # For simplicity, let's mask out half of the joints\n",
    "# # Shape: (batch_size, seq_len, num_joints)\n",
    "# mask = torch.tensor([[[1] * 9 + [0] * 9]])\n",
    "\n",
    "# # Run the forward pass with the test inputs and mask\n",
    "# with torch.no_grad():\n",
    "#     output_embeddings = embedding_layer(joint_positions, velocities, accelerations, mask)\n",
    "\n",
    "# # Check if the masked positions in the output embeddings are set to zero\n",
    "# masked_output = output_embeddings[:, :, mask.view(-1) == 0]\n",
    "# assert torch.all(masked_output == 0), \"Masked positions in the output are not all zero!\"\n",
    "\n",
    "# print(\"Masking applied correctly:\", torch.all(masked_output == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        src: Tensor of shape (batch_size, seq_len, num_joints, embed_dim)\n",
    "        src_mask: None or Tensor for masking in multi-head attention (not used in this example)\n",
    "        src_key_padding_mask: Tensor of shape (batch_size, seq_len * num_joints) indicating which elements are padded\n",
    "        \"\"\"\n",
    "        # Reshaping src to fit the transformer's input requirement\n",
    "        \n",
    "        batch_size, seq_len, num_joints, embed_dim = src.size()\n",
    "        src = src.view(batch_size, seq_len * num_joints, embed_dim)  # Flatten seq_len and num_joints\n",
    "\n",
    "        # Applying Transformer Encoder\n",
    "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Reshape back to (batch_size, seq_len, num_joints, embed_dim)\n",
    "        output = output.view(batch_size, seq_len, num_joints, embed_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# num_joints = 18\n",
    "# dof = 3\n",
    "# embed_dim = 128\n",
    "# seq_len = 60\n",
    "# num_heads = 4\n",
    "# num_layers = 3\n",
    "# batch_size = 10\n",
    "\n",
    "# # Create instances\n",
    "# input_embedding = SkeletalInputEmbedding(num_joints=num_joints, dof=dof, embed_dim=embed_dim, seq_len=seq_len)\n",
    "# transformer_encoder = TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "\n",
    "# # Generate synthetic joint positions, velocities, accelerations\n",
    "# joint_positions = torch.randn(batch_size, seq_len, num_joints, dof)\n",
    "# velocities = torch.randn(batch_size, seq_len, num_joints, dof)\n",
    "# accelerations = torch.randn(batch_size, seq_len, num_joints, dof)\n",
    "\n",
    "# # Generate mask (1 for data present, 0 for data missing)\n",
    "# mask = torch.randint(0, 2, (batch_size, seq_len, num_joints)).float()  # Random binary mask\n",
    "\n",
    "# def test_transformer_encoder():\n",
    "#     # Embedding inputs\n",
    "#     embeddings = input_embedding(joint_positions, velocities, accelerations, mask=mask)\n",
    "#     print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "#     # Apply encoder\n",
    "#     # Note: src_key_padding_mask needs to be reshaped properly to match the expected dimensions in the encoder\n",
    "#     src_key_padding_mask = mask.view(batch_size, seq_len * num_joints)\n",
    "#     output = transformer_encoder(embeddings, src_key_padding_mask=1-src_key_padding_mask)\n",
    "#     print(\"Encoder output shape:\", output.shape)\n",
    "\n",
    "#     # Check output shape\n",
    "#     assert output.shape == (batch_size, seq_len, num_joints, embed_dim), \"Output shape mismatch\"\n",
    "#     print(\"Test passed!\")\n",
    "\n",
    "# def test_transformer_encoder_with_masking():\n",
    "#     # Embedding inputs\n",
    "#     embeddings = input_embedding(joint_positions, velocities, accelerations, mask=mask)\n",
    "#     print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "#     # Create an all-ones mask (no data is considered missing)\n",
    "#     no_mask = torch.ones_like(mask)\n",
    "\n",
    "#     # Apply encoder with actual mask\n",
    "#     src_key_padding_mask = mask.view(batch_size, seq_len * num_joints)\n",
    "#     masked_output = transformer_encoder(embeddings, src_key_padding_mask=1-src_key_padding_mask)\n",
    "#     print(\"Masked Encoder output shape:\", masked_output.shape)\n",
    "\n",
    "#     # Apply encoder without any mask (all data is considered present)\n",
    "#     no_mask_key_padding_mask = no_mask.view(batch_size, seq_len * num_joints)\n",
    "#     unmasked_output = transformer_encoder(embeddings, src_key_padding_mask=1-no_mask_key_padding_mask)\n",
    "#     print(\"Unmasked Encoder output shape:\", unmasked_output.shape)\n",
    "\n",
    "#     # Check output shapes\n",
    "#     assert masked_output.shape == (batch_size, seq_len, num_joints, embed_dim), \"Output shape mismatch with mask\"\n",
    "#     assert unmasked_output.shape == (batch_size, seq_len, num_joints, embed_dim), \"Output shape mismatch without mask\"\n",
    "\n",
    "#     # Check if outputs are different (which they should be if masking is effective)\n",
    "#     difference = torch.abs(masked_output - unmasked_output).sum()\n",
    "#     print(\"Difference between masked and unmasked outputs:\", difference.item())\n",
    "#     assert difference > 0, \"Mask seems to have no effect\"\n",
    "\n",
    "#     print(\"Masking functionality test passed!\")\n",
    "\n",
    "# # Run the test\n",
    "# test_transformer_encoder_with_masking()\n",
    "\n",
    "\n",
    "# # Run the test\n",
    "# test_transformer_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, num_joints, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_joints = num_joints\n",
    "\n",
    "        # Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        # Output layer to convert decoder output to joint position dimension\n",
    "        self.output_layer = nn.Linear(self.embed_dim, 3)  # Assuming output per joint is a 3D position\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: Tensor of shape (batch_size, output_seq_len, num_joints, embed_dim), initially could be start token or zero vectors\n",
    "        memory: Tensor of shape (batch_size, input_seq_len * num_joints, embed_dim), output from the Transformer encoder\n",
    "        tgt_mask: Mask to ensure the decoder's predictions are based only on past positions\n",
    "        memory_mask: Optional, to mask encoder outputs if necessary\n",
    "        tgt_key_padding_mask: Tensor of shape (batch_size, output_seq_len * num_joints) for masking target sequences\n",
    "        memory_key_padding_mask: Tensor of shape (batch_size, input_seq_len * num_joints) for masking memory sequences\n",
    "        \"\"\"\n",
    "        # Reshaping memory and target to fit the transformer's input requirement\n",
    "        batch_size, input_seq_len, num_joints, embed_dim = memory.size()\n",
    "        batch_size, output_seq_len, num_joints, embed_dim = tgt.size()\n",
    "        memory = memory.view(batch_size, input_seq_len * num_joints, embed_dim)  # Ensure memory is correctly reshaped\n",
    "        tgt = tgt.view(batch_size, output_seq_len * num_joints, embed_dim)  # Flatten output_seq_len and num_joints\n",
    "\n",
    "        # Transformer Decoder\n",
    "        output = self.transformer_decoder(\n",
    "            tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "        # Reshape back and project to joint position dimensions\n",
    "        output = output.view(batch_size, output_seq_len, num_joints, embed_dim)\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize the decoder\n",
    "# embed_dim = 128\n",
    "# num_heads = 8\n",
    "# num_layers = 4\n",
    "# num_joints = 18\n",
    "# dropout_rate = 0.1\n",
    "# decoder = TransformerDecoder(embed_dim, num_heads, num_layers, num_joints, dropout_rate)\n",
    "\n",
    "# # Mock data setup\n",
    "# batch_size = 5\n",
    "# seq_len = 10  # Length of the output sequence\n",
    "# input_seq_len = 10  # Length of the encoder output sequence\n",
    "# tgt = torch.rand(batch_size, seq_len, num_joints, embed_dim)  # Random target for simulation\n",
    "# memory = torch.rand(batch_size, input_seq_len, num_joints, embed_dim)  # Encoder output\n",
    "\n",
    "# # Masks setup\n",
    "# tgt_key_padding_mask = torch.zeros(batch_size, seq_len * num_joints, dtype=torch.bool)\n",
    "# memory_key_padding_mask = torch.zeros(batch_size, input_seq_len * num_joints, dtype=torch.bool)\n",
    "\n",
    "# # Simulate some missing data\n",
    "# tgt_key_padding_mask[0, 50:] = True\n",
    "# memory_key_padding_mask[0, 50:] = True\n",
    "\n",
    "# # Causal mask to prevent looking ahead in the target sequence\n",
    "# tgt_mask = torch.triu(torch.ones(seq_len * num_joints, seq_len * num_joints), diagonal=1).bool()\n",
    "\n",
    "# # Forward pass through the decoder\n",
    "# output = decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "# # Check outputs\n",
    "# print(\"Output shape:\", output.shape)  # Expected: (batch_size, seq_len, num_joints, 3)\n",
    "\n",
    "# # Check if padding affects the output\n",
    "# print(\"Output for padded data (should be zero or unchanged):\")\n",
    "# print(output[0, -1])  # Check last few outputs of the first batch where padding is applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(norm_pos, norm_vel, norm_acc, mask, input_length=60, predict_length=60):\n",
    "    num_frames = norm_pos.shape[0]\n",
    "    num_joints = norm_pos.shape[1]\n",
    "\n",
    "    # Calculate the total number of sequences we can create\n",
    "    num_sequences = num_frames - input_length - predict_length + 1\n",
    "\n",
    "    # Initialize arrays to store the input and target sequences\n",
    "    X_pos = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_vel = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_acc = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    Y_pos = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_vel = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_acc = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    X_mask = np.zeros((num_sequences, input_length, num_joints))\n",
    "    Y_mask = np.zeros((num_sequences, predict_length, num_joints))\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(num_sequences):\n",
    "        X_pos[i] = norm_pos[i:i + input_length]\n",
    "        X_vel[i] = norm_vel[i:i + input_length]\n",
    "        X_acc[i] = norm_acc[i:i + input_length]\n",
    "        Y_pos[i] = norm_pos[i + input_length:i + input_length + predict_length]\n",
    "        Y_vel[i] = norm_vel[i + input_length:i + input_length + predict_length]\n",
    "        Y_acc[i] = norm_acc[i + input_length:i + input_length + predict_length]\n",
    "        X_mask[i] = mask[i:i + input_length]\n",
    "        Y_mask[i] = mask[i + input_length:i + input_length + predict_length]\n",
    "\n",
    "    return X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "batch_size=1\n",
    "seq_length=60\n",
    "num_joints=18\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, batch_size=1, seq_len=60, num_joints=18, dof=3, embed_dim=128, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embedding = SkeletalInputEmbedding(num_joints=num_joints, dof=dof, embed_dim=embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_encoder_layers)\n",
    "        self.decoder = TransformerDecoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_encoder_layers, num_joints=num_joints, dropout_rate=dropout)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, batch_size, seq_len, num_joints, xpos, xvel, xacc, xmask, ypos, yvel, yacc, ymask):\n",
    "        inputembeddings = self.input_embedding(xpos, xvel, xacc, xmask)\n",
    "        tgtembeddings = self.input_embedding(ypos, yvel, yacc, ymask) \n",
    "\n",
    "        #print(inputembeddings)\n",
    "\n",
    "        src_key_padding_mask = ~xmask.view(batch_size, seq_len * num_joints)\n",
    "        tgt_key_padding_mask = ~ymask.view(batch_size, seq_len * num_joints)\n",
    "        \n",
    "        encoder_output = self.encoder(inputembeddings,src_key_padding_mask=src_key_padding_mask)\n",
    "        # Teacher forcing: use true input embeddings as decoder input during training\n",
    "        decoder_output = self.decoder(tgtembeddings, encoder_output, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n",
    "        return decoder_output\n",
    "\n",
    "model = TransformerModel()\n",
    "model.cuda()\n",
    "\n",
    "# Setup DataLoaders\n",
    "\n",
    "X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask = generate_sequences(norm_pos, norm_vel, norm_acc, masks)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_pos_tensor = torch.tensor(X_pos, dtype=torch.float32)\n",
    "X_vel_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "X_acc_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "X_mask_tensor = torch.tensor(X_mask, dtype=torch.bool)\n",
    "\n",
    "Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "Y_vel_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "Y_acc_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "Y_mask_tensor = torch.tensor(Y_mask, dtype=torch.bool)\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = TensorDataset(X_pos_tensor, X_vel_tensor, X_acc_tensor, X_mask_tensor, Y_pos_tensor, Y_vel_tensor, Y_acc_tensor, Y_mask_tensor)\n",
    "\n",
    "# Contiguous split based on time\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "train_dataset = Subset(full_dataset, range(train_size))\n",
    "val_dataset = Subset(full_dataset, range(train_size, len(full_dataset)))\n",
    "\n",
    "# Creating the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Custom Loss Function\n",
    "def custom_masked_mse_loss(output, target, xmask, ymask):\n",
    "    # Expand mask dimensions to match the dimensions of output and target\n",
    "    xmask_expanded = xmask.unsqueeze(-1).expand_as(output)  # Ensure it covers the last dimension as well\n",
    "    ymask_expanded = ymask.unsqueeze(-1).expand_as(target)\n",
    "    \n",
    "    # Apply the masks\n",
    "    output = output * xmask_expanded\n",
    "    target = target * ymask_expanded\n",
    "    \n",
    "    # Compute the difference and the squared difference\n",
    "    diff = output - target\n",
    "    squared_diff = diff ** 2\n",
    "    \n",
    "    # Apply the mask again to the squared differences\n",
    "    masked_squared_diff = squared_diff[xmask_expanded.bool()]\n",
    "    \n",
    "    # Return the mean of the masked squared differences, or zero if empty\n",
    "    return masked_squared_diff.mean() if masked_squared_diff.numel() > 0 else torch.tensor(0.0).to(output.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Training and validation\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_loss = float('inf')\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xpos, xvel, xacc, xmask, ypos, yvel, yacc, ymask, in train_dataloader:\n",
    "        xpos, xvel, xacc, xmask = xpos.cuda(), xvel.cuda(), xacc.cuda(), xmask.cuda()\n",
    "        ypos, yvel, yacc, ymask = ypos.cuda(), yvel.cuda(), yacc.cuda(), ymask.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_size, seq_length, num_joints, xpos, xvel, xacc, xmask, ypos, yvel, yacc, ymask)\n",
    "\n",
    "        loss = custom_masked_mse_loss(output, ypos, xmask, ymask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_train_loss = total_loss / len(train_dataloader)\n",
    "    train_losses.append(average_train_loss)\n",
    "\n",
    "    # model.eval()\n",
    "    # total_val_loss = 0.0\n",
    "    # with torch.no_grad():\n",
    "    #     for pos, vel, acc, mask, y_pos in val_dataloader:\n",
    "    #         pos, vel, acc, mask = pos.cuda(), vel.cuda(), acc.cuda(), mask.cuda()\n",
    "    #         y_pos = y_pos.cuda()\n",
    "\n",
    "    #         output = model(pos, vel, acc, mask)\n",
    "    #         val_loss = criterion(output, y_pos, mask)\n",
    "    #         total_val_loss += val_loss.item()\n",
    "\n",
    "    # average_val_loss = total_val_loss / len(val_dataloader)\n",
    "    # val_losses.append(average_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Training Loss: {average_train_loss:.4f}') \n",
    "   \n",
    "\n",
    "    if average_train_loss < best_loss:\n",
    "        best_loss = average_train_loss\n",
    "        torch.save(model.state_dict(), 'best_model_weights.pth')\n",
    "        print(f'Saved new best model with validation loss: {average_val_loss:.4f}')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "\n",
    "# class TransformerModel(nn.Module):\n",
    "#     def __init__(self, num_joints=18, dof=3, embed_dim=128, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         self.input_embedding = SkeletalInputEmbedding(num_joints=num_joints, dof=dof, embed_dim=embed_dim)\n",
    "#         self.encoder = TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "#         self.decoder = TransformerDecoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "#         self._initialize_weights()\n",
    "    \n",
    "#     def _initialize_weights(self):\n",
    "#         for module in self.modules():\n",
    "#             if isinstance(module, nn.Linear):\n",
    "#                 torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if module.bias is not None:\n",
    "#                     torch.nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "#     def forward(self, pos, vel, acc, mask):\n",
    "#         embeddings = self.input_embedding(pos, vel, acc, mask)\n",
    "#         # if torch.isnan(embeddings).any():\n",
    "#         #print(embeddings[0][0])\n",
    " \n",
    "    \n",
    "#         encoder_output = self.encoder(embeddings)\n",
    "#         # if torch.isnan(encoder_output).any():\n",
    "#         #     print(\"NaN detected in encoder output\")\n",
    "    \n",
    "#         predicted_output = self.decoder(encoder_output, encoder_output)\n",
    "#         # if torch.isnan(predicted_output).any():\n",
    "#         #     print(predicted_output)\n",
    "        \n",
    "#         return predicted_output\n",
    "\n",
    "# # Assuming the previous imports and TransformerModel definition\n",
    "# model = TransformerModel()\n",
    "# model.cuda()\n",
    "\n",
    "# X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask = generate_sequences(norm_pos, norm_vel, norm_acc, masks)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_pos_tensor = torch.tensor(X_pos, dtype=torch.float32)\n",
    "# X_vel_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "# X_acc_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "# X_mask_tensor = torch.tensor(X_mask, dtype=torch.bool)\n",
    "# Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "\n",
    "# # Create the full dataset\n",
    "# full_dataset = TensorDataset(X_pos_tensor, X_vel_tensor, X_acc_tensor, X_mask_tensor, Y_pos_tensor)\n",
    "\n",
    "# # Contiguous split based on time\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# train_dataset = Subset(full_dataset, range(train_size))\n",
    "# val_dataset = Subset(full_dataset, range(train_size, len(full_dataset)))\n",
    "\n",
    "# # Creating the DataLoaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)  # Shuffling is generally not done in time-series\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# # Loss and Optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Lists for storing loss values\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# # Training loop\n",
    "# best_loss = float('inf')\n",
    "# num_epochs = 50\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for pos, vel, acc, mask, y_pos in train_dataloader:\n",
    "#         pos, vel, acc, mask = pos.cuda(), vel.cuda(), acc.cuda(), mask.cuda()\n",
    "#         y_pos = y_pos.cuda()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(pos, vel, acc, mask)\n",
    "#         #print(mask.shape)\n",
    "#         mask = mask.unsqueeze(-1).expand_as(output)\n",
    "#         output = output.where(~torch.isnan(output), torch.zeros_like(output))\n",
    "\n",
    "#         y_pos = y_pos.where(~torch.isnan(y_pos), torch.zeros_like(y_pos))\n",
    "\n",
    "#         masked_output = output * mask\n",
    "#         masked_y_pos = y_pos * mask\n",
    "\n",
    "#         loss = criterion(masked_output, masked_y_pos)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_dataloader)\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     print(f'Epoch {epoch+1}, Training Loss: {epoch_loss}')\n",
    "\n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     val_running_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for pos, vel, acc, mask, y_pos in val_dataloader:\n",
    "#             pos, vel, acc, mask = pos.cuda(), vel.cuda(), acc.cuda(), mask.cuda()\n",
    "#             y_pos = y_pos.cuda()\n",
    "\n",
    "#             output = model(pos, vel, acc, mask)\n",
    "            \n",
    "#             mask = mask.unsqueeze(-1).expand_as(output)\n",
    "#             output = output.where(~torch.isnan(output), torch.zeros_like(output))\n",
    "\n",
    "#             y_pos = y_pos.where(~torch.isnan(y_pos), torch.zeros_like(y_pos))\n",
    "\n",
    "#             masked_output = output * mask\n",
    "#             masked_y_pos = y_pos * mask\n",
    "\n",
    "#             val_loss = criterion(masked_output, masked_y_pos)\n",
    "#             val_running_loss += val_loss.item()\n",
    "\n",
    "#     val_epoch_loss = val_running_loss / len(val_dataloader)\n",
    "#     val_losses.append(val_epoch_loss)\n",
    "#     print(f'Epoch {epoch+1}, Validation Loss: {val_epoch_loss}')\n",
    "\n",
    "#     # Check if the validation loss improved\n",
    "#     if val_epoch_loss < best_loss:\n",
    "#         best_loss = val_epoch_loss\n",
    "#         torch.save(model.state_dict(), 'best_model_weights.pth')\n",
    "#         print(f'Epoch {epoch+1}, New Best Validation Loss: {val_epoch_loss}, Model Saved')\n",
    "\n",
    "        \n",
    "\n",
    "# print(\"Training complete\")\n",
    "\n",
    "# # Plotting the training and validation losses\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(train_losses, label='Training Loss')\n",
    "# plt.plot(val_losses, label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss per Epoch')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "    print(\"Weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load weights:\", e)\n",
    "\n",
    "\n",
    "# Check for NaNs or extreme values in weights\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"NaN found in {name}\")\n",
    "    if param.abs().max() > 1e6:  # Example threshold for \"extreme\" values\n",
    "        print(f\"Extreme values found in {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_joints=18, dof=3, embed_dim=128, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embedding = SkeletalInputEmbedding(num_joints=num_joints, dof=dof, embed_dim=embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_encoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.decoder = TransformerDecoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_decoder_layers, dim_feedforward=dim_feedforward, num_joints=num_joints, dof=dof, dropout=dropout)\n",
    "    #     self._initialize_weights()\n",
    "\n",
    "    # def _initialize_weights(self):\n",
    "    #     for module in self.modules():\n",
    "    #         if isinstance(module, nn.Linear):\n",
    "    #             torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "    #             if module.bias is not None:\n",
    "    #                 torch.nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, pos, vel, acc, mask):\n",
    "        embeddings = self.input_embedding(pos, vel, acc, mask)\n",
    "        # if torch.isnan(embeddings).any():\n",
    "        #print(embeddings)\n",
    "\n",
    "        encoder_output = self.encoder(embeddings)\n",
    "        # if torch.isnan(encoder_output).any():\n",
    "        #     print(\"NaN detected in encoder output\")\n",
    "\n",
    "        predicted_output = self.decoder(encoder_output, encoder_output, num_frames_to_predict=60)\n",
    "        # if torch.isnan(predicted_output).any():\n",
    "        #     print(predicted_output)\n",
    "        \n",
    "        return predicted_output\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = TransformerModel()\n",
    "model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "model.cuda()  # Ensure model is in CUDA environment for GPU usage\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Generate or load your test dataset here\n",
    "# For demonstration, I'm assuming you have a function to generate or load test data similar to your training setup\n",
    "X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask = generate_sequences(norm_pos, norm_vel, norm_acc, masks)\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_pos_test_tensor = torch.tensor(X_pos, dtype=torch.float32)\n",
    "X_vel_test_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "X_acc_test_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "X_mask_test_tensor = torch.tensor(X_mask, dtype=torch.bool)\n",
    "\n",
    "# Create a Dataset and DataLoader for testing\n",
    "test_dataset = TensorDataset(X_pos_test_tensor, X_vel_test_tensor, X_acc_test_tensor, X_mask_test_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)  # Batch size set to 1 for testing one sequence at a time\n",
    "\n",
    "# Testing loop\n",
    "for pos, vel, acc, mask in test_dataloader:\n",
    "    pos, vel, acc, mask = pos.cuda(), vel.cuda(), acc.cuda(), mask.cuda()\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        predicted_output = model(pos, vel, acc, mask)\n",
    "    \n",
    "    # Now, `predicted_output` contains the model's predictions for the next 60 frames\n",
    "    # Depending on your setup, you might want to apply any post-processing or visualization here\n",
    "    # For example, print the output or compare it with ground truth data if available\n",
    "    print(predicted_output.cpu().numpy())  # Output predictions to CPU and convert to numpy for printing or further analysis\n",
    "\n",
    "    break\n",
    "\n",
    "# Add any specific metrics or visualizations you need to evaluate the model's predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class TransformerModel(nn.Module):\n",
    "#     def __init__(self, num_joints=18, dof=3, embed_dim=128, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         self.input_embedding = SkeletalInputEmbedding(num_joints=num_joints, dof=dof, embed_dim=embed_dim)\n",
    "#         self.encoder = TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_encoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "#         self.decoder = TransformerDecoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_decoder_layers, dim_feedforward=dim_feedforward, num_joints=num_joints, dof=dof, dropout=dropout)\n",
    "#         self._initialize_weights()\n",
    "\n",
    "#     def _initialize_weights(self):\n",
    "#         for module in self.modules():\n",
    "#             if isinstance(module, nn.Linear):\n",
    "#                 torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if module.bias is not None:\n",
    "#                     torch.nn.init.constant_(module.bias, 0)\n",
    "\n",
    "#     def forward(self, pos, vel, acc, mask):\n",
    "#         embeddings = self.input_embedding(pos, vel, acc, mask)\n",
    "#         # if torch.isnan(embeddings).any():\n",
    "#         #print(embeddings)\n",
    "\n",
    "#         encoder_output = self.encoder(embeddings)\n",
    "#         # if torch.isnan(encoder_output).any():\n",
    "#         #     print(\"NaN detected in encoder output\")\n",
    "\n",
    "#         predicted_output = self.decoder(encoder_output, encoder_output, num_frames_to_predict=60)\n",
    "#         # if torch.isnan(predicted_output).any():\n",
    "#         #     print(predicted_output)\n",
    "        \n",
    "#         return predicted_output\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_pos_tensor = torch.tensor(X_pos, dtype=torch.float32)\n",
    "# X_vel_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "# X_acc_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "# X_mask_tensor = torch.tensor(X_mask, dtype=torch.bool)\n",
    "\n",
    "# Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "\n",
    "# dataset = TensorDataset(X_pos_tensor, X_vel_tensor, X_acc_tensor, X_mask_tensor, Y_pos_tensor)\n",
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# # Initialize the model\n",
    "# model = TransformerModel()\n",
    "# model.cuda()  # Assuming you're using GPU\n",
    "# #register_hooks(model)\n",
    "\n",
    "# # Loss and Optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop\n",
    "\n",
    "# best_loss = float('inf')\n",
    "# num_epochs = 50\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for pos, vel, acc, mask, y_pos in dataloader:\n",
    "#         pos, vel, acc, mask = pos.cuda(), vel.cuda(), acc.cuda(), mask.cuda()\n",
    "#         y_pos = y_pos.cuda()\n",
    "    \n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(pos, vel, acc, mask)\n",
    "#         #print(output)\n",
    "    \n",
    "#         mask = mask.unsqueeze(-1).expand_as(output)\n",
    "#         output = output.where(~torch.isnan(output), torch.zeros_like(output))\n",
    "        \n",
    "#         y_pos = y_pos.where(~torch.isnan(y_pos), torch.zeros_like(y_pos))\n",
    "        \n",
    "#         masked_output = output * mask\n",
    "#         masked_y_pos = y_pos * mask\n",
    "#         # print(\"output: \", output[0][0])\n",
    "#         # print(\"mask: \", mask[0][0])\n",
    "#         # print(\"masked output: \", masked_output[0][0])\n",
    "        \n",
    "#         loss = criterion(masked_output, masked_y_pos)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     # for name, param in model.named_parameters():\n",
    "#     #     if torch.isnan(param).any():\n",
    "#     #         print(f\"Pre-save NaN found in {name}\")\n",
    "#     #     else:\n",
    "#     #         print(f\"{name} - max: {param.max()}, min: {param.min()}, mean: {param.mean()}\")\n",
    "\n",
    "#     epoch_loss = running_loss / len(dataloader)\n",
    "#     if epoch_loss < best_loss:\n",
    "#         best_loss = epoch_loss\n",
    "#         torch.save(model.state_dict(), 'best_model_weights.pth')  # Save the best model weights\n",
    "#         print(f'Epoch {epoch+1}, Loss: {epoch_loss} (new best, model saved)')\n",
    "#     else:\n",
    "#         print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n",
    "\n",
    "# print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033b1eb5-8e73-46fa-97e8-30208fb91eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from embedding_layers import SkeletalInputEmbedding\n",
    "from encoder_layers import TransformerEncoder\n",
    "from decoder_layers import TransformerDecoder\n",
    "import TF_helper_functions as hf\n",
    "import pickle\n",
    "import ipyvolume as ipv\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb28bfa-65b6-4023-a035-35036b5aa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath='/home/maleen/research_data/Transformers/datasets/training/'\n",
    "# Base path and file information\n",
    "base_name='24_07_25_training_norm'\n",
    "\n",
    "weights_path='/home/maleen/research_data/Transformers/models/TF_tokenised/24_07_31_v1_best_model.pth'\n",
    "\n",
    "filename=datapath+base_name+'.pkl'\n",
    "\n",
    "def load_results_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def process_all_datasets(results, input_length=60, predict_length=60):\n",
    "    all_X_pos, all_X_vel, all_X_acc = [], [], []\n",
    "    all_Y_pos, all_Y_vel, all_Y_acc = [], [], []\n",
    "    discarded_frames = {}\n",
    "\n",
    "    for i in range(1, 7):  # Assuming you have 6 datasets\n",
    "        dataset_key = f'dataset{i}'\n",
    "        norm_pos = results[f'{dataset_key}_normpos']\n",
    "        norm_vel = results[f'{dataset_key}_normvel']\n",
    "        norm_acc = results[f'{dataset_key}_normacc']\n",
    "\n",
    "        # Generate sequences for this dataset\n",
    "        X_pos, X_vel, X_acc, Y_pos, Y_vel, Y_acc = hf.generate_sequences(norm_pos, norm_vel, norm_acc, input_length, predict_length)\n",
    "        \n",
    "        all_X_pos.append(X_pos)\n",
    "        all_X_vel.append(X_vel)\n",
    "        all_X_acc.append(X_acc)\n",
    "        all_Y_pos.append(Y_pos)\n",
    "        all_Y_vel.append(Y_vel)\n",
    "        all_Y_acc.append(Y_acc)\n",
    "\n",
    "        # Calculate discarded frames\n",
    "        total_frames = norm_pos.shape[0]\n",
    "        used_frames = X_pos.shape[0] + input_length + predict_length - 1\n",
    "        discarded = total_frames - used_frames\n",
    "        discarded_frames[dataset_key] = discarded\n",
    "\n",
    "    # Combine sequences from all datasets\n",
    "    combined_X_pos = np.concatenate(all_X_pos)\n",
    "    combined_X_vel = np.concatenate(all_X_vel)\n",
    "    combined_X_acc = np.concatenate(all_X_acc)\n",
    "    combined_Y_pos = np.concatenate(all_Y_pos)\n",
    "    combined_Y_vel = np.concatenate(all_Y_vel)\n",
    "    combined_Y_acc = np.concatenate(all_Y_acc)\n",
    "\n",
    "    return (combined_X_pos, combined_X_vel, combined_X_acc, \n",
    "            combined_Y_pos, combined_Y_vel, combined_Y_acc, \n",
    "            discarded_frames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b76f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sequences shapes:\n",
      "X_pos shape: (7559, 30, 6, 3)\n",
      "X_vel shape: (7559, 30, 6, 3)\n",
      "X_acc shape: (7559, 30, 6, 3)\n",
      "Y_pos shape: (7559, 20, 6, 3)\n",
      "Y_vel shape: (7559, 20, 6, 3)\n",
      "Y_acc shape: (7559, 20, 6, 3)\n",
      "\n",
      "Discarded frames per dataset:\n",
      "dataset1: 0 frames\n",
      "dataset2: 0 frames\n",
      "dataset3: 0 frames\n",
      "dataset4: 0 frames\n",
      "dataset5: 0 frames\n",
      "dataset6: 0 frames\n"
     ]
    }
   ],
   "source": [
    "input_length = 30\n",
    "predict_length = 20\n",
    "datasetnum=6\n",
    "\n",
    "# Load the results\n",
    "results = load_results_from_pickle(filename)\n",
    "\n",
    "medians_pos = results['combined_medians_pos']\n",
    "iqrs_pos = results['combined_iqrs_pos']\n",
    "\n",
    "\n",
    "# Process all datasets and get combined sequences\n",
    "(combined_X_pos, combined_X_vel, combined_X_acc, \n",
    " combined_Y_pos, combined_Y_vel, combined_Y_acc, \n",
    " discarded_frames) = process_all_datasets(results, input_length, predict_length)\n",
    "\n",
    "print(\"Combined sequences shapes:\")\n",
    "print(f\"X_pos shape: {combined_X_pos.shape}\")\n",
    "print(f\"X_vel shape: {combined_X_vel.shape}\")\n",
    "print(f\"X_acc shape: {combined_X_acc.shape}\")\n",
    "print(f\"Y_pos shape: {combined_Y_pos.shape}\")\n",
    "print(f\"Y_vel shape: {combined_Y_vel.shape}\")\n",
    "print(f\"Y_acc shape: {combined_Y_acc.shape}\")\n",
    "\n",
    "print(\"\\nDiscarded frames per dataset:\")\n",
    "for dataset, frames in discarded_frames.items():\n",
    "    print(f\"{dataset}: {frames} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8abd09",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.5163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.3987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.2614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.5446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(0.6202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(1.3929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 6, 3])\n",
      "tensor(1.7349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Predicted Positions: [[[ 2.0442255   0.81462294  2.5527415 ]\n",
      "  [ 2.7632782   1.6324916   3.0203073 ]\n",
      "  [ 2.855541    1.7081611   3.7018692 ]\n",
      "  [ 3.2957954   1.5126333   3.4965715 ]\n",
      "  [ 3.3050659  -0.10339123  2.9203367 ]\n",
      "  [ 2.8523893  -0.9240758   2.4249036 ]]\n",
      "\n",
      " [[ 1.9999536   0.77861834  2.5063846 ]\n",
      "  [ 2.7773523   1.5844582   2.9649606 ]\n",
      "  [ 2.8427641   1.7006024   3.6467092 ]\n",
      "  [ 3.2658596   1.4991337   3.4185717 ]\n",
      "  [ 3.3910453  -0.17300172  2.8290045 ]\n",
      "  [ 2.9674265  -0.96942395  2.3653936 ]]\n",
      "\n",
      " [[ 2.0409064   0.7866495   2.5249183 ]\n",
      "  [ 2.774374    1.6169565   2.9773974 ]\n",
      "  [ 2.8379397   1.7457594   3.666265  ]\n",
      "  [ 3.2782183   1.5439155   3.4479923 ]\n",
      "  [ 3.3869615  -0.17917718  2.8519542 ]\n",
      "  [ 2.92623    -0.93491876  2.3830621 ]]\n",
      "\n",
      " [[ 2.047431    0.7614053   2.5313654 ]\n",
      "  [ 2.7700222   1.612644    2.9950845 ]\n",
      "  [ 2.8314223   1.7294298   3.7085862 ]\n",
      "  [ 3.2760057   1.5430958   3.482975  ]\n",
      "  [ 3.3852     -0.17966266  2.8585787 ]\n",
      "  [ 2.9635127  -0.98734695  2.3680434 ]]\n",
      "\n",
      " [[ 2.0289562   0.7824978   2.5251033 ]\n",
      "  [ 2.775346    1.5985228   2.9832108 ]\n",
      "  [ 2.828456    1.7176328   3.6696095 ]\n",
      "  [ 3.2491128   1.5224637   3.4556172 ]\n",
      "  [ 3.3435128  -0.15996522  2.8665407 ]\n",
      "  [ 2.9197943  -0.95185775  2.3963556 ]]\n",
      "\n",
      " [[ 2.0256078   0.7660939   2.5299234 ]\n",
      "  [ 2.7680132   1.6297553   3.0087192 ]\n",
      "  [ 2.828628    1.7255533   3.7293365 ]\n",
      "  [ 3.2500184   1.5633097   3.516755  ]\n",
      "  [ 3.2889047  -0.10235825  2.8997636 ]\n",
      "  [ 2.9086533  -0.98923045  2.413972  ]]\n",
      "\n",
      " [[ 2.0233839   0.7486893   2.536391  ]\n",
      "  [ 2.773079    1.5747722   2.9966717 ]\n",
      "  [ 2.8091419   1.6888456   3.705375  ]\n",
      "  [ 3.2242405   1.519707    3.5111735 ]\n",
      "  [ 3.291924   -0.09735046  2.9184606 ]\n",
      "  [ 2.9068441  -0.9498268   2.444401  ]]\n",
      "\n",
      " [[ 2.020296    0.78545934  2.5222795 ]\n",
      "  [ 2.7692611   1.5980923   2.9877732 ]\n",
      "  [ 2.8190212   1.6998937   3.6773055 ]\n",
      "  [ 3.2207224   1.5049139   3.4769537 ]\n",
      "  [ 3.2912278  -0.1350825   2.8979464 ]\n",
      "  [ 2.8879814  -0.9473528   2.4280949 ]]\n",
      "\n",
      " [[ 2.0125232   0.7827206   2.5298574 ]\n",
      "  [ 2.7605796   1.6318533   3.0075805 ]\n",
      "  [ 2.824252    1.7115575   3.711253  ]\n",
      "  [ 3.2333636   1.549762    3.5166912 ]\n",
      "  [ 3.2723548  -0.06382741  2.9176753 ]\n",
      "  [ 2.894947   -0.9585261   2.4462333 ]]\n",
      "\n",
      " [[ 2.0224032   0.78074133  2.528109  ]\n",
      "  [ 2.7618845   1.6116773   2.9915962 ]\n",
      "  [ 2.8174791   1.7091691   3.6941907 ]\n",
      "  [ 3.234954    1.5231955   3.4815662 ]\n",
      "  [ 3.3314245  -0.15113388  2.8799016 ]\n",
      "  [ 2.9373388  -0.9685523   2.4171119 ]]\n",
      "\n",
      " [[ 2.0406306   0.7830419   2.5381548 ]\n",
      "  [ 2.7754219   1.6250883   3.0112827 ]\n",
      "  [ 2.8279943   1.7185315   3.7141576 ]\n",
      "  [ 3.238527    1.546183    3.5203607 ]\n",
      "  [ 3.2670033  -0.10137931  2.9203427 ]\n",
      "  [ 2.880363   -0.9582335   2.4381568 ]]\n",
      "\n",
      " [[ 2.0251524   0.75504243  2.5319192 ]\n",
      "  [ 2.7728696   1.5854278   3.006908  ]\n",
      "  [ 2.8194358   1.6903162   3.7251983 ]\n",
      "  [ 3.2311506   1.515141    3.5162709 ]\n",
      "  [ 3.2652216  -0.09233922  2.914473  ]\n",
      "  [ 2.89062    -0.9828182   2.4248598 ]]\n",
      "\n",
      " [[ 2.0287118   0.7375865   2.498668  ]\n",
      "  [ 2.8322198   1.5587567   2.9789908 ]\n",
      "  [ 2.854896    1.6767383   3.681054  ]\n",
      "  [ 3.2377756   1.5604236   3.5048842 ]\n",
      "  [ 3.2692459   0.04318006  2.967487  ]\n",
      "  [ 2.874728   -0.88898736  2.4805832 ]]\n",
      "\n",
      " [[ 1.9594804   0.7700918   2.5217907 ]\n",
      "  [ 2.7055166   1.6395577   3.0023594 ]\n",
      "  [ 2.8173444   1.725613    3.7058976 ]\n",
      "  [ 3.2556863   1.5725539   3.5324886 ]\n",
      "  [ 3.1601484   0.03216247  2.941171  ]\n",
      "  [ 2.7719643  -0.9523252   2.4278476 ]]\n",
      "\n",
      " [[ 1.7819383   0.6576827   2.2919357 ]\n",
      "  [ 2.5715933   1.626382    2.7792184 ]\n",
      "  [ 2.7263422   1.7637287   3.5078397 ]\n",
      "  [ 3.200712    1.647493    3.3376544 ]\n",
      "  [ 2.9217048   0.16962887  2.703981  ]\n",
      "  [ 2.544407   -1.0128152   2.0858843 ]]\n",
      "\n",
      " [[ 1.978844    0.73032606  2.5140705 ]\n",
      "  [ 2.7427185   1.6157967   3.0008779 ]\n",
      "  [ 2.806926    1.7370603   3.6986072 ]\n",
      "  [ 3.2327244   1.6051836   3.5348608 ]\n",
      "  [ 3.1079943   0.06393422  2.9450796 ]\n",
      "  [ 2.7396805  -0.9607236   2.3962033 ]]\n",
      "\n",
      " [[ 2.0278132   0.7361651   2.5413318 ]\n",
      "  [ 2.7901843   1.5725892   3.009287  ]\n",
      "  [ 2.813704    1.692011    3.706308  ]\n",
      "  [ 3.203012    1.5277625   3.5536895 ]\n",
      "  [ 3.1582427  -0.02897223  3.0003803 ]\n",
      "  [ 2.763323   -0.8837989   2.4819977 ]]\n",
      "\n",
      " [[ 1.9859222   0.7339713   2.5140471 ]\n",
      "  [ 2.7664955   1.5806214   2.998907  ]\n",
      "  [ 2.8379185   1.6944319   3.7176697 ]\n",
      "  [ 3.2529068   1.5295441   3.536202  ]\n",
      "  [ 3.179439   -0.01064653  2.9602344 ]\n",
      "  [ 2.7924428  -0.95815694  2.4371579 ]]\n",
      "\n",
      " [[ 2.0280368   0.8036154   2.5529342 ]\n",
      "  [ 2.7309704   1.6470945   3.021903  ]\n",
      "  [ 2.8165822   1.7293806   3.7293377 ]\n",
      "  [ 3.243793    1.4998511   3.5282123 ]\n",
      "  [ 3.231146   -0.18465893  2.91279   ]\n",
      "  [ 2.8310778  -0.9835368   2.4131358 ]]\n",
      "\n",
      " [[ 1.9252278   0.7336368   2.4476461 ]\n",
      "  [ 2.6463752   1.6496694   2.9269662 ]\n",
      "  [ 2.7792494   1.7818195   3.6623409 ]\n",
      "  [ 3.2536047   1.5738583   3.4590974 ]\n",
      "  [ 3.0720334  -0.15498324  2.8113089 ]\n",
      "  [ 2.6711318  -1.0515444   2.2322419 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the saved model weights\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_joints = 6\n",
    "dropout_rate = 0.1\n",
    "autoregressiveloops=20\n",
    "batch_size = 1\n",
    "dof=3\n",
    "input_dim = num_joints * dof\n",
    "\n",
    "# Initialize the models with the same configuration as during training\n",
    "embedding = SkeletalInputEmbedding(input_dim).to(device)\n",
    "#t_embedding = TargetEmbedding(num_joints=num_joints, dof=3, embed_dim=embed_dim,device=device).to(device)\n",
    "encoder = TransformerEncoder(embed_dim, num_heads, num_layers, dropout_rate).to(device)\n",
    "decoder = TransformerDecoder(embed_dim, num_heads, num_layers, num_joints, dropout_rate).to(device)\n",
    "\n",
    "# Load state dicts\n",
    "embedding.load_state_dict(checkpoint['embedding_state_dict'])\n",
    "#t_embedding.load_state_dict(checkpoint['t_embedding_state_dict'])\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "embedding.eval()\n",
    "#t_embedding.eval()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# X_pos, X_vel, X_acc, Y_pos, Y_vel, Y_acc = generate_sequences(norm_pos, norm_vel, norm_acc, input_length, predict_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_pos_tensor = torch.tensor(combined_X_pos, dtype=torch.float32)\n",
    "X_vel_tensor = torch.tensor(combined_X_vel, dtype=torch.float32)\n",
    "X_acc_tensor = torch.tensor(combined_X_acc, dtype=torch.float32)\n",
    "\n",
    "\n",
    "Y_pos_tensor = torch.tensor(combined_Y_pos, dtype=torch.float32)\n",
    "Y_vel_tensor = torch.tensor(combined_Y_vel, dtype=torch.float32)\n",
    "Y_acc_tensor = torch.tensor(combined_Y_acc, dtype=torch.float32)\n",
    "\n",
    "# Create the DataLoader for inference data\n",
    "dataset = TensorDataset(X_pos_tensor, X_vel_tensor, X_acc_tensor, Y_pos_tensor, Y_vel_tensor)\n",
    "inference_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prepare for autoregressive decoding\n",
    "predicted_positions = []\n",
    "\n",
    "criterion = hf.MaskedMSELoss()\n",
    "\n",
    "# Perform inference across all batches\n",
    "for batch in inference_loader:\n",
    "    X_pos_batch, X_vel_batch, X_acc_batch, Y_pos_batch, Y_vel_tensor = [b.to(device) for b in batch]\n",
    "\n",
    "    # Encoder pass\n",
    "   \n",
    "    inputembeddings = embedding(X_pos_batch, X_vel_batch)\n",
    "    memory = encoder(inputembeddings, src_key_padding_mask=None)\n",
    "\n",
    "    \n",
    "    # Initialize the start token for decoding\n",
    "    current_pos = X_pos_batch[:, -1:, :, :]\n",
    "    current_vel= X_vel_batch[:, -1:, :, :]\n",
    "\n",
    "\n",
    "    for i in range(autoregressiveloops):\n",
    "        # Embed the current position\n",
    "        Y_expected= Y_pos_batch[:,i:i+1,:,:]\n",
    "    \n",
    "\n",
    "        # # #Running whole model\n",
    "        # if i > 0:\n",
    "        #     X_mask_batch_ar = torch.cat([X_mask_batch[:, 1:, :], current_mask], dim=1)\n",
    "        #     X_pos_batch_ar = torch.cat([X_pos_batch[:, 1:, :, :], current_pos], dim=1)\n",
    "    \n",
    "        #     src_key_padding_mask = ~X_mask_batch_ar.view(batch_size, input_length * num_joints)\n",
    "        #     input_embeddings = embedding(X_pos_batch_ar, X_mask_batch_ar)\n",
    "        #     memory = encoder(input_embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "        # ##\n",
    "        \n",
    "        current_embeddings = embedding(current_pos, current_vel)\n",
    "        \n",
    "        # Decoder pass\n",
    "        output = decoder(current_embeddings, memory, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "        print(output.shape)\n",
    "    \n",
    "        # Update current_pos for the next prediction\n",
    "\n",
    "        old_pos= current_pos\n",
    "        current_pos = output[:, :, :, :].detach()  # only take the last timestep\n",
    "\n",
    "        #current_vel = (current_pos-old_pos)/0.1\n",
    "\n",
    "        # velocity_error=Y_vel_tensor[:, i, :, :]-current_vel\n",
    "        current_vel = Y_vel_tensor[:, i, :, :]\n",
    "        #\n",
    "        # print('velocity_error: ', velocity_error)\n",
    "    \n",
    "        predicted_positions.append(current_pos.squeeze().cpu().numpy())\n",
    "\n",
    "        \n",
    "    \n",
    "        output = output.where(~torch.isnan(output), torch.zeros_like(output))\n",
    "        # masked_output = output * Xmask\n",
    "    \n",
    "        Y_expected = Y_expected.where(~torch.isnan(Y_expected), torch.zeros_like(Y_expected))\n",
    "        # masked_y_pos = Y_pos_batch * Ymask\n",
    "\n",
    "        #print(Y_expected)\n",
    "    \n",
    "        # Compute loss\n",
    "        \n",
    "        loss = criterion(output, Y_expected)\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "    break\n",
    "# Convert the list of predicted positions to a more manageable form, e.g., a NumPy array\n",
    "predicted_positions = np.array(predicted_positions)\n",
    "\n",
    "print(\"Predicted Positions:\", predicted_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe4a82bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed5d037b8db4870bdd0f78fbd5e108f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Frame', max=19)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20108f5e5a54134834b03e51cd40c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Container(figure=Figure(box_center=[0.5, 0.5, 0.5], box_size=[1.0, 1.0, 1.0], camera=PerspectiveCamera(fov=45.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updated_connections = [\n",
    "        (0, 1), (1, 2),  # Right arm\n",
    "        (3, 4), (4, 5),  # Left arm\n",
    "        (2, 3),  # Connection between arms\n",
    "    ]\n",
    "\n",
    "# Animation function\n",
    "def update_plot(pred):\n",
    "    # First dataset processing\n",
    "    data = hf.reverse_normalization(predicted_positions[pred], medians_pos, iqrs_pos)\n",
    "\n",
    "    if len(data.shape) == 1:\n",
    "        print(\"Data is a scalar or has unexpected shape.\")\n",
    "        return np.zeros((1, 3)), [], np.zeros((1, 3)), []\n",
    "    else:\n",
    "        valid_keypoints = ~np.isnan(data[:, :3]).any(axis=1)\n",
    "        filtered_data = data[valid_keypoints]\n",
    "\n",
    "        # Create mapping from old indices to new indices after NaN removal\n",
    "        index_mapping = {old_index: new_index for new_index, old_index in enumerate(np.where(valid_keypoints)[0])}\n",
    "        # Create new connections for the first dataset\n",
    "        new_connections = [(index_mapping[start], index_mapping[end])\n",
    "                           for start, end in updated_connections\n",
    "                           if start in index_mapping and end in index_mapping]\n",
    "\n",
    "    # Second dataset processing\n",
    "    data_y = hf.reverse_normalization(Y_pos[0][pred], medians_pos, iqrs_pos)\n",
    "\n",
    "    if len(data_y.shape) == 1:\n",
    "        print(\"Data_y is a scalar or has unexpected shape.\")\n",
    "        return np.zeros((1, 3)), [], np.zeros((1, 3)), []\n",
    "    else:\n",
    "        valid_keypoints_y = ~np.isnan(data_y[:, :3]).any(axis=1)\n",
    "        filtered_data_y = data_y[valid_keypoints_y]\n",
    "\n",
    "        # Create mapping from old indices to new indices for the second dataset\n",
    "        index_mapping_y = {old_index: new_index for new_index, old_index in enumerate(np.where(valid_keypoints_y)[0])}\n",
    "        # Create new connections for the second dataset\n",
    "        new_connections_y = [(index_mapping_y[start], index_mapping_y[end])\n",
    "                             for start, end in updated_connections\n",
    "                             if start in index_mapping_y and end in index_mapping_y]\n",
    "\n",
    "    return filtered_data, new_connections, filtered_data_y, new_connections_y\n",
    "\n",
    "# Plot configuration\n",
    "ipv.figure()\n",
    "\n",
    "# Initialize scatter and plot objects with some initial data\n",
    "scatter = ipv.scatter([0], [0], [0], color='blue', marker='sphere', size=2)\n",
    "scatter_y = ipv.scatter([0], [0], [0], color='green', marker='sphere', size=2)\n",
    "lines = [ipv.plot([0, 0], [0, 0], [0, 0], color='red') for _ in range(len(updated_connections))]\n",
    "lines_y = [ipv.plot([0, 0], [0, 0], [0, 0], color='lime') for _ in range(len(updated_connections))]\n",
    "\n",
    "# Set axis limits\n",
    "ipv.xlim(-1, 1)\n",
    "ipv.ylim(-1, 1)\n",
    "ipv.zlim(-1, 1)\n",
    "\n",
    "def animate(pred):\n",
    "    filtered_data, new_connections, filtered_data_y, new_connections_y = update_plot(pred)\n",
    "    \n",
    "    if len(filtered_data) == 0 or len(filtered_data_y) == 0:\n",
    "        return\n",
    "    \n",
    "    # Update scatter data\n",
    "    scatter.x = filtered_data[:, 0]\n",
    "    scatter.y = filtered_data[:, 1]\n",
    "    scatter.z = filtered_data[:, 2]\n",
    "    \n",
    "    scatter_y.x = filtered_data_y[:, 0]\n",
    "    scatter_y.y = filtered_data_y[:, 1]\n",
    "    scatter_y.z = filtered_data_y[:, 2]\n",
    "    \n",
    "    # Update lines data\n",
    "    for line, (start, end) in zip(lines, new_connections):\n",
    "        line.x = filtered_data[[start, end], 0]\n",
    "        line.y = filtered_data[[start, end], 1]\n",
    "        line.z = filtered_data[[start, end], 2]\n",
    "    \n",
    "    for line_y, (start, end) in zip(lines_y, new_connections_y):\n",
    "        line_y.x = filtered_data_y[[start, end], 0]\n",
    "        line_y.y = filtered_data_y[[start, end], 1]\n",
    "        line_y.z = filtered_data_y[[start, end], 2]\n",
    "\n",
    "# Define the range for pred\n",
    "pred_range = range(len(predicted_positions))  # Adjust this range according to your data\n",
    "\n",
    "# Create slider\n",
    "slider = widgets.IntSlider(min=0, max=len(pred_range) - 1, step=1, description='Frame')\n",
    "\n",
    "# Update plot when slider value changes\n",
    "widgets.interactive(animate, pred=slider)\n",
    "\n",
    "# Display the slider and plot\n",
    "display(slider)\n",
    "ipv.view(azimuth=0, elevation=-90)\n",
    "ipv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e640f7a-312f-4e42-aa9c-72f09b3f59ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

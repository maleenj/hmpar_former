{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ipyvolume as ipv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileroot='2024_03_08_TRF_Maleen/'\n",
    "filename='2024_03_08_TRF_Maleen_01'\n",
    "\n",
    "datapath='/home/maleen/research_data/Transformers/datasets/training/'\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "df1 = pd.read_pickle(datapath + filename + '.pkl')\n",
    "\n",
    "arrays = [np.array(item) for item in df1['Skeleton_3D']]\n",
    "timestamps = [np.array(item) for item in df1['Skeleton_Timestamp']]\n",
    "\n",
    "# Convert datetime to seconds from start\n",
    "timestamps = (timestamps - timestamps[0])\n",
    "\n",
    "# Stack these arrays along a new axis to create a 3D NumPy array\n",
    "# Each \"slice\" of this 3D array represents one frame of keypoints\n",
    "skeleton_3d_frames = np.stack(arrays, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create masks for the data (1 = data present, 0 = data missing)\n",
    "# Correcting the mask values\n",
    "masks = np.where(np.isnan(skeleton_3d_frames).any(axis=2), 0, 1)  # 0 for missing, 1 for present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the differences in position and time\n",
    "position_diff = np.diff(skeleton_3d_frames, axis=0)\n",
    "time_diff = np.diff(timestamps)\n",
    "\n",
    "# Ensure that time_diff is of shape (n,1,1) so that it broadcasts correctly when dividing\n",
    "time_diff = time_diff[:, np.newaxis, np.newaxis]\n",
    "\n",
    "# Update masks to match the dimensions of position_diff and time_diff\n",
    "# We use the bitwise AND operator to ensure that both the current and previous frames are valid\n",
    "masks_pos = masks[:-1, :] & masks[1:, :]\n",
    "\n",
    "# Add an additional dimension to masks\n",
    "masks_pos = masks_pos[:,:,np.newaxis]\n",
    "\n",
    "# Now we calculate velocity, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_vel = np.where(masks_pos, position_diff / time_diff, np.nan)\n",
    "\n",
    "masks_velocity = masks_pos[:-1, :] & masks_pos[1:, :]\n",
    "\n",
    "# Calculate the differences in velocity\n",
    "velocity_diff = np.diff(skel_vel, axis=0)\n",
    "\n",
    "# Now we calculate acceleration, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_acc = np.where(masks_velocity, velocity_diff / time_diff[:-1, :, :], np.nan)  # Use time_diff with one less time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, slice skeleton_3d_frames and skel_vel to match the dimensions of skell_acc\n",
    "skel_pos= skeleton_3d_frames[2:]\n",
    "skel_vel = skel_vel[1:]\n",
    "masks = masks[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to hold the normalized data, medians, and IQRs\n",
    "norm_pos = np.empty_like(skel_pos)\n",
    "medians_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "iqrs_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "\n",
    "norm_vel= np.empty_like(skel_vel)\n",
    "medians_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "iqrs_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "\n",
    "norm_acc = np.empty_like(skel_acc)\n",
    "medians_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "iqrs_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "\n",
    "def robust_normalize_data_with_clipping(data, masks, medians_per_joint_axis, iqrs_per_joint_axis, normalized_data, clipping_percentiles=(1, 99)):\n",
    "    for joint in range(data.shape[1]):  # For each joint\n",
    "        for axis in range(data.shape[2]):  # For each axis (x, y, z)\n",
    "            joint_axis_data = data[:, joint, axis]\n",
    "            mask_for_joint = masks[:, joint]\n",
    "\n",
    "            # Select valid data based on the mask\n",
    "            valid_data = joint_axis_data[mask_for_joint == 1]\n",
    "\n",
    "            # Determine clipping thresholds based on percentiles\n",
    "            lower_threshold, upper_threshold = np.percentile(valid_data, clipping_percentiles) if valid_data.size > 0 else (np.nan, np.nan)\n",
    "\n",
    "            # Clip the data based on valid mask and thresholds\n",
    "            clipped_values = np.clip(joint_axis_data, lower_threshold, upper_threshold)\n",
    "\n",
    "            # Calculate median and IQR for clipped data\n",
    "            median = np.median(clipped_values[mask_for_joint == 1]) if np.any(mask_for_joint == 1) else np.nan\n",
    "            q75, q25 = np.percentile(clipped_values[mask_for_joint == 1], [75 ,25]) if np.any(mask_for_joint == 1) else (np.nan, np.nan)\n",
    "            iqr = q75 - q25\n",
    "\n",
    "            # Store the calculated medians and IQRs\n",
    "            medians_per_joint_axis[joint, axis] = median\n",
    "            iqrs_per_joint_axis[joint, axis] = iqr\n",
    "\n",
    "            # Normalize the clipped data, avoiding division by zero\n",
    "            if iqr > 0:\n",
    "                normalized_values = (clipped_values - median) / iqr\n",
    "            else:\n",
    "                normalized_values = clipped_values  # Keep original values if IQR is 0 or nan\n",
    "\n",
    "            # Apply normalization only where data is present\n",
    "            normalized_data[:, joint, axis] = np.where(mask_for_joint == 1, normalized_values, np.nan)\n",
    "            \n",
    "    return normalized_data, medians_per_joint_axis, iqrs_per_joint_axis\n",
    "\n",
    "# Example usage with your data\n",
    "# Note: masks, skel_pos, skel_vel, skel_acc should be defined in your context\n",
    "\n",
    "norm_pos, medians_pos, iqrs_pos = robust_normalize_data_with_clipping(skel_pos, masks, medians_per_joint_axis_skel_pos, iqrs_per_joint_axis_skel_pos, norm_pos)\n",
    "norm_vel, medians_vel, iqrs_vel = robust_normalize_data_with_clipping(skel_vel, masks, medians_per_joint_axis_vel, iqrs_per_joint_axis_vel, norm_vel)\n",
    "norm_acc, medians_acc, iqrs_acc = robust_normalize_data_with_clipping(skel_acc, masks, medians_per_joint_axis_acc, iqrs_per_joint_axis_acc, norm_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of normalized data: -1.4094628242311558e-18\n",
      "Data points within [-1, 1] (IQR): 925 out of 1018\n",
      "Data points within [-1, 1] (IQR): 688 out of 1018\n"
     ]
    }
   ],
   "source": [
    "data=skel_acc\n",
    "norm_data=norm_acc\n",
    "# Plot the original and normalized data for a specific joint and axis\n",
    "joint, axis = 0, 0  # Change as needed\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[:, joint, axis], bins=20, alpha=0.7, label='Original')\n",
    "plt.title(\"Original Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(norm_data[:, joint, axis], bins=20, alpha=0.7, label='Normalized')\n",
    "plt.title(\"Normalized Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check the median and range of the normalized data\n",
    "normalized_median = np.nanmedian(norm_data[:, joint, axis])\n",
    "print(\"Median of normalized data:\", normalized_median)\n",
    "\n",
    "within_iqr = ((norm_data[:, joint, axis] > -2) & (norm_data[:, joint, axis] < 2)).sum()\n",
    "print(f\"Data points within [-1, 1] (IQR): {within_iqr} out of {norm_data.shape[0]}\")\n",
    "\n",
    "within_iqr2 = ((data[:, joint, axis] > -1) & (data[:, joint, axis] < 1)).sum()\n",
    "print(f\"Data points within [-1, 1] (IQR): {within_iqr2} out of {norm_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletalInputEmbedding(nn.Module):\n",
    "    def __init__(self, num_joints=18, dof=3, embed_dim=128, device='gpu'):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.dof = dof\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device  # Store the device\n",
    "\n",
    "        self.joint_embed = nn.Linear(dof, embed_dim)\n",
    "        # self.vel_embed = nn.Linear(dof, embed_dim)\n",
    "        # self.acc_embed = nn.Linear(dof, embed_dim)\n",
    "\n",
    "    #def forward(self, joint_positions, velocities, accelerations, mask=None):\n",
    "    def forward(self, joint_positions, mask=None):\n",
    "        # Replace NaNs in the input data\n",
    "        joint_positions = torch.nan_to_num(joint_positions)\n",
    "        # velocities = torch.nan_to_num(velocities)\n",
    "        # accelerations = torch.nan_to_num(accelerations)\n",
    "\n",
    "\n",
    "        # Embedding and combining the embeddings\n",
    "        joint_embeddings = self.joint_embed(joint_positions)\n",
    "        # vel_embeddings = self.vel_embed(velocities)\n",
    "        # acc_embeddings = self.acc_embed(accelerations)\n",
    "        combined_embeddings = joint_embeddings #+ vel_embeddings + acc_embeddings\n",
    "\n",
    "        # Apply mask after NaN replacement\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)  # Add a dimension for the features\n",
    "            combined_embeddings = combined_embeddings * mask\n",
    "\n",
    "        # Calculate positional encoding dynamically\n",
    "        seq_len = joint_positions.size(1)  # Assuming joint_positions is [seq_len, num_joints, dof]\n",
    "        positional_encoding = self.get_sinusoidal_encoding(seq_len * self.num_joints, self.embed_dim)\n",
    "        # print(\"Device of input embeddings:\", combined_embeddings.device)\n",
    "        # print(\"Device of positional encodings:\", positional_encoding.device)\n",
    "\n",
    "        positional_encoding = positional_encoding.view(seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        combined_embeddings += positional_encoding.unsqueeze(0)  # Unsqueeze to add batch dimension for broadcasting\n",
    "        combined_embeddings = combined_embeddings.view(-1, seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        return combined_embeddings\n",
    "\n",
    "    def get_sinusoidal_encoding(self, total_len, embed_dim):\n",
    "        # Make sure the position tensor is created on the right device\n",
    "        position = torch.arange(0, total_len, dtype=torch.float, device=self.device).unsqueeze(1)\n",
    "    \n",
    "        # Calculate the division term for sinusoidal encoding\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim)).to(self.device)\n",
    "        div_term = div_term.unsqueeze(0)  # Reshape for broadcasting\n",
    "    \n",
    "        # Create the positional encoding matrix\n",
    "        pe = torch.zeros(total_len, embed_dim, device=self.device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "        return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEmbedding(nn.Module):\n",
    "    def __init__(self, num_joints=18, dof=3, embed_dim=128, device='gpu'):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.dof = dof\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device  # Store the device\n",
    "\n",
    "        self.joint_embed = nn.Linear(dof, embed_dim)\n",
    "        # self.vel_embed = nn.Linear(dof, embed_dim)\n",
    "        # self.acc_embed = nn.Linear(dof, embed_dim)\n",
    "\n",
    "    #def forward(self, joint_positions, velocities, accelerations, mask=None):\n",
    "    def forward(self, joint_positions, mask=None):\n",
    "        # Replace NaNs in the input data\n",
    "        joint_positions = torch.nan_to_num(joint_positions)\n",
    "        # velocities = torch.nan_to_num(velocities)\n",
    "        # accelerations = torch.nan_to_num(accelerations)\n",
    "\n",
    "\n",
    "        # Embedding and combining the embeddings\n",
    "        joint_embeddings = self.joint_embed(joint_positions)\n",
    "        # vel_embeddings = self.vel_embed(velocities)\n",
    "        # acc_embeddings = self.acc_embed(accelerations)\n",
    "        combined_embeddings = joint_embeddings #+ vel_embeddings + acc_embeddings\n",
    "\n",
    "        # Apply mask after NaN replacement\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)  # Add a dimension for the features\n",
    "            combined_embeddings = combined_embeddings * mask\n",
    "\n",
    "        # Calculate positional encoding dynamically\n",
    "        seq_len = joint_positions.size(1)  # Assuming joint_positions is [seq_len, num_joints, dof]\n",
    "        positional_encoding = self.get_sinusoidal_encoding(seq_len * self.num_joints, self.embed_dim)\n",
    "        # print(\"Device of input embeddings:\", combined_embeddings.device)\n",
    "        # print(\"Device of positional encodings:\", positional_encoding.device)\n",
    "\n",
    "        positional_encoding = positional_encoding.view(seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        combined_embeddings += positional_encoding.unsqueeze(0)  # Unsqueeze to add batch dimension for broadcasting\n",
    "        combined_embeddings = combined_embeddings.view(-1, seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        return combined_embeddings\n",
    "\n",
    "    def get_sinusoidal_encoding(self, total_len, embed_dim):\n",
    "        # Make sure the position tensor is created on the right device\n",
    "        position = torch.arange(0, total_len, dtype=torch.float, device=self.device).unsqueeze(1)\n",
    "    \n",
    "        # Calculate the division term for sinusoidal encoding\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim)).to(self.device)\n",
    "        div_term = div_term.unsqueeze(0)  # Reshape for broadcasting\n",
    "    \n",
    "        # Create the positional encoding matrix\n",
    "        pe = torch.zeros(total_len, embed_dim, device=self.device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "        return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        src: Tensor of shape (batch_size, seq_len, num_joints, embed_dim)\n",
    "        src_mask: None or Tensor for masking in multi-head attention (not used in this example)\n",
    "        src_key_padding_mask: Tensor of shape (batch_size, seq_len * num_joints) indicating which elements are padded\n",
    "        \"\"\"\n",
    "        # Reshaping src to fit the transformer's input requirement\n",
    "        \n",
    "        batch_size, seq_len, num_joints, embed_dim = src.size()\n",
    "        src = src.view(batch_size, seq_len * num_joints, embed_dim)  # Flatten seq_len and num_joints\n",
    "\n",
    "        #print(\"SRC shape:\", src.shape)\n",
    "\n",
    "        # Applying Transformer Encoder\n",
    "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Reshape back to (batch_size, seq_len, num_joints, embed_dim)\n",
    "        output = output.view(batch_size, seq_len, num_joints, embed_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, num_joints, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_joints = num_joints\n",
    "\n",
    "        # Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        # Output layer to convert decoder output to joint position dimension\n",
    "        self.output_layer = nn.Linear(self.embed_dim, 3)  # Assuming output per joint is a 3D position\n",
    "\n",
    "    # @staticmethod\n",
    "    # def generate_square_subsequent_mask(sz):\n",
    "    #     mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    #     mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    #     return mask\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: Tensor of shape (batch_size, output_seq_len, num_joints, embed_dim), initially could be start token or zero vectors\n",
    "        memory: Tensor of shape (batch_size, input_seq_len * num_joints, embed_dim), output from the Transformer encoder\n",
    "        tgt_mask: Mask to ensure the decoder's predictions are based only on past positions\n",
    "        memory_mask: Optional, to mask encoder outputs if necessary\n",
    "        tgt_key_padding_mask: Tensor of shape (batch_size, output_seq_len * num_joints) for masking target sequences\n",
    "        memory_key_padding_mask: Tensor of shape (batch_size, input_seq_len * num_joints) for masking memory sequences\n",
    "        \"\"\"\n",
    "        # Reshaping memory and target to fit the transformer's input requirement\n",
    "        batch_size, input_seq_len, num_joints, embed_dim = memory.size()\n",
    "        batch_size, output_seq_len, num_joints, embed_dim = tgt.size()\n",
    "        memory = memory.view(batch_size, input_seq_len * num_joints, embed_dim)  # Ensure memory is correctly reshaped\n",
    "        tgt = tgt.view(batch_size, output_seq_len * num_joints, embed_dim)  # Flatten output_seq_len and num_joints\n",
    "\n",
    "        # Transformer Decoder\n",
    "        output = self.transformer_decoder(\n",
    "            tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "        # Reshape back and project to joint position dimensions\n",
    "        output = output.view(batch_size, output_seq_len, num_joints, embed_dim)\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(norm_pos, norm_vel, norm_acc, mask, input_length=60, predict_length=60):\n",
    "    num_frames = norm_pos.shape[0]\n",
    "    num_joints = norm_pos.shape[1]\n",
    "\n",
    "    # Calculate the total number of sequences we can create\n",
    "    num_sequences = num_frames - input_length - predict_length + 1\n",
    "\n",
    "    # Initialize arrays to store the input and target sequences\n",
    "    X_pos = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_vel = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_acc = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    Y_pos = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_vel = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_acc = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    X_mask = np.zeros((num_sequences, input_length, num_joints))\n",
    "    Y_mask = np.zeros((num_sequences, predict_length, num_joints))\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(num_sequences):\n",
    "        X_pos[i] = norm_pos[i:i + input_length]\n",
    "        X_vel[i] = norm_vel[i:i + input_length]\n",
    "        X_acc[i] = norm_acc[i:i + input_length]\n",
    "        Y_pos[i] = norm_pos[i + input_length:i + input_length + predict_length]\n",
    "        Y_vel[i] = norm_vel[i + input_length:i + input_length + predict_length]\n",
    "        Y_acc[i] = norm_acc[i + input_length:i + input_length + predict_length]\n",
    "        X_mask[i] = mask[i:i + input_length]\n",
    "        Y_mask[i] = mask[i + input_length:i + input_length + predict_length]\n",
    "\n",
    "    return X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask\n",
    "\n",
    "\n",
    "def create_shifted_mask(seq_length, num_joints):\n",
    "    # seq_length is the number of time steps\n",
    "    # num_joints is the number of joints per time step\n",
    "    total_length = seq_length * num_joints\n",
    "    mask = torch.ones((total_length, total_length), dtype=torch.float32) * float('-inf')  # Start with everything masked\n",
    "    for i in range(seq_length):\n",
    "        for j in range(i + 1):  # Allow visibility up to and including the current time step\n",
    "            start_row = i * num_joints\n",
    "            end_row = start_row + num_joints\n",
    "            start_col = j * num_joints\n",
    "            end_col = start_col + num_joints\n",
    "            mask[start_row:end_row, start_col:end_col] = 0.0  # Unmask the allowed region\n",
    "\n",
    "    return mask\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target, output_mask, target_mask):\n",
    "        # Ensure that the masks are applied, and only the relevant positions are used in loss calculation\n",
    "        masked_output = output * output_mask\n",
    "        masked_target = target * target_mask\n",
    "\n",
    "        # Compute the squared differences\n",
    "        squared_diff = (masked_output - masked_target) ** 2\n",
    "\n",
    "        # Apply masks to the squared differences\n",
    "        # This step is technically not necessary if the masks for output and target are the same\n",
    "        masked_squared_diff = squared_diff * output_mask * target_mask\n",
    "\n",
    "        # Calculate the mean of the squared differences while avoiding division by zero\n",
    "        # This only considers the number of elements that are included in the mask\n",
    "        loss = masked_squared_diff.sum() / (output_mask * target_mask).sum().clamp(min=1)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 18, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted Positions: [[[-8.11877966e-01 -7.92822838e-01 -2.65573263e-01]\n",
      "  [-7.30861306e-01 -8.25014055e-01 -6.20516539e-01]\n",
      "  [-2.89994657e-01 -8.68384182e-01 -2.29963049e-01]\n",
      "  [ 9.78232548e-03 -8.30083430e-01 -1.59342989e-01]\n",
      "  [ 5.82032464e-02 -7.49183238e-01 -5.55215068e-02]\n",
      "  [-8.69428396e-01 -7.82927454e-01 -1.11669588e+00]\n",
      "  [-7.19642401e-01 -8.09005558e-01 -2.14479282e-01]\n",
      "  [-7.58949876e-01 -6.95926130e-01  9.46635306e-01]\n",
      "  [-5.00347376e-01 -7.96798766e-01  1.93559483e-01]\n",
      "  [ 2.37903282e-01 -3.10163170e-01 -3.67072999e-01]\n",
      "  [ 2.73874074e-01 -2.52924532e-01 -3.00535232e-01]\n",
      "  [-8.26813698e-01 -8.16338003e-01 -4.95190978e-01]\n",
      "  [ 1.24580950e-01 -1.77658454e-01 -1.18648186e-01]\n",
      "  [ 7.14753419e-02 -1.56260923e-01 -1.70141146e-01]\n",
      "  [-7.33139575e-01 -7.68708169e-01 -2.00839296e-01]\n",
      "  [-7.19969153e-01 -7.86173820e-01 -4.09405410e-01]\n",
      "  [-5.71188927e-01 -8.57411563e-01 -3.45654756e-01]\n",
      "  [-8.52641225e-01 -6.46076620e-01 -5.82354367e-01]]\n",
      "\n",
      " [[-7.62899578e-01 -7.39927471e-01 -1.44913614e-01]\n",
      "  [-6.45017982e-01 -7.63425410e-01 -5.28400064e-01]\n",
      "  [-2.28032723e-01 -8.23889315e-01 -3.08079720e-01]\n",
      "  [ 1.17627621e-01 -7.72899091e-01 -1.84779182e-01]\n",
      "  [ 1.37126014e-01 -6.92210972e-01 -4.24642153e-02]\n",
      "  [-8.16925168e-01 -7.19161510e-01 -9.64672446e-01]\n",
      "  [-7.84811974e-01 -7.32775509e-01 -5.35911657e-02]\n",
      "  [-8.17159772e-01 -6.23755574e-01  1.25610924e+00]\n",
      "  [-4.45552051e-01 -7.39545584e-01  1.03363201e-01]\n",
      "  [ 2.11732224e-01 -3.18793625e-01 -3.92534494e-01]\n",
      "  [ 2.60477215e-01 -2.56432861e-01 -3.10293436e-01]\n",
      "  [-7.92296052e-01 -7.78762162e-01 -1.30997226e-01]\n",
      "  [ 1.07276291e-01 -1.81352809e-01 -1.07937187e-01]\n",
      "  [ 5.27622961e-02 -1.63243845e-01 -1.73095018e-01]\n",
      "  [-6.96731329e-01 -6.94868505e-01 -7.58164525e-02]\n",
      "  [-6.22704387e-01 -7.15959251e-01 -3.02657336e-01]\n",
      "  [-5.29825151e-01 -7.77767360e-01 -3.65868092e-01]\n",
      "  [-7.70506144e-01 -5.81432045e-01 -4.49552059e-01]]\n",
      "\n",
      " [[-7.01910377e-01 -7.15420306e-01 -3.56184579e-02]\n",
      "  [-5.43144166e-01 -7.41446674e-01 -4.80263948e-01]\n",
      "  [-1.38211295e-01 -8.04793000e-01 -4.05031055e-01]\n",
      "  [ 2.41534397e-01 -7.43989766e-01 -2.23337099e-01]\n",
      "  [ 2.31749460e-01 -6.64150417e-01 -5.39195798e-02]\n",
      "  [-7.50210524e-01 -6.86271846e-01 -8.47119391e-01]\n",
      "  [-8.35894465e-01 -6.75435245e-01  1.66283831e-01]\n",
      "  [-8.66457939e-01 -5.72981775e-01  1.54623985e+00]\n",
      "  [-3.82581860e-01 -7.10879147e-01  4.25322019e-02]\n",
      "  [ 1.99185342e-01 -3.26395005e-01 -3.95673811e-01]\n",
      "  [ 2.55054921e-01 -2.61332184e-01 -3.03289562e-01]\n",
      "  [-7.45200753e-01 -7.59819329e-01  9.43093747e-02]\n",
      "  [ 9.01602954e-02 -1.87314346e-01 -8.66150409e-02]\n",
      "  [ 3.32343392e-02 -1.68014482e-01 -1.65469334e-01]\n",
      "  [-6.47454739e-01 -6.60735905e-01  4.27470766e-02]\n",
      "  [-5.19621968e-01 -6.87276959e-01 -2.39727035e-01]\n",
      "  [-4.64596629e-01 -7.39513695e-01 -3.83602709e-01]\n",
      "  [-6.95406199e-01 -5.49696922e-01 -3.53413761e-01]]\n",
      "\n",
      " [[-6.34352803e-01 -7.09463954e-01  6.43846393e-02]\n",
      "  [-4.33195621e-01 -7.40486085e-01 -4.62123185e-01]\n",
      "  [-3.22282501e-02 -7.95668125e-01 -4.96569097e-01]\n",
      "  [ 3.71206969e-01 -7.33838260e-01 -2.59622991e-01]\n",
      "  [ 3.33889514e-01 -6.53365910e-01 -8.24499130e-02]\n",
      "  [-6.76575303e-01 -6.75404251e-01 -7.41505027e-01]\n",
      "  [-8.80472600e-01 -6.29820824e-01  4.53341842e-01]\n",
      "  [-9.00228977e-01 -5.43155909e-01  1.78888059e+00]\n",
      "  [-3.18048030e-01 -7.06609726e-01  3.94664332e-03]\n",
      "  [ 1.82121128e-01 -3.35491031e-01 -3.85861248e-01]\n",
      "  [ 2.46565714e-01 -2.65762657e-01 -2.85365343e-01]\n",
      "  [-7.03047752e-01 -7.59930134e-01  2.57423818e-01]\n",
      "  [ 6.95988387e-02 -1.96370974e-01 -5.49757667e-02]\n",
      "  [ 1.21666379e-02 -1.75581947e-01 -1.46739677e-01]\n",
      "  [-5.93044698e-01 -6.51752770e-01  1.51319310e-01]\n",
      "  [-4.12652642e-01 -6.85160100e-01 -2.12501571e-01]\n",
      "  [-3.86969417e-01 -7.27857828e-01 -4.06562030e-01]\n",
      "  [-6.28411531e-01 -5.41069567e-01 -2.73494542e-01]]\n",
      "\n",
      " [[-5.63068151e-01 -7.15239882e-01  1.53408617e-01]\n",
      "  [-3.25149059e-01 -7.50631154e-01 -4.65913296e-01]\n",
      "  [ 7.52899647e-02 -7.95030594e-01 -5.73254943e-01]\n",
      "  [ 4.89909798e-01 -7.35460639e-01 -2.81681418e-01]\n",
      "  [ 4.32430804e-01 -6.54976368e-01 -1.10204294e-01]\n",
      "  [-6.02762640e-01 -6.77597344e-01 -6.44124091e-01]\n",
      "  [-9.28901970e-01 -5.91149271e-01  7.99204111e-01]\n",
      "  [-9.19877887e-01 -5.32993257e-01  1.98440266e+00]\n",
      "  [-2.56460756e-01 -7.21396744e-01 -1.69997700e-02]\n",
      "  [ 1.59824088e-01 -3.44306380e-01 -3.61118346e-01]\n",
      "  [ 2.32204199e-01 -2.68739790e-01 -2.57215530e-01]\n",
      "  [-6.68278098e-01 -7.75245905e-01  3.78609091e-01]\n",
      "  [ 4.57262136e-02 -2.05867067e-01 -2.01504864e-02]\n",
      "  [-9.19315219e-03 -1.84039846e-01 -1.19081810e-01]\n",
      "  [-5.38709283e-01 -6.59240961e-01  2.50336081e-01]\n",
      "  [-3.06450874e-01 -6.98073506e-01 -2.20470294e-01]\n",
      "  [-3.08833867e-01 -7.28645205e-01 -4.31694835e-01]\n",
      "  [-5.69193482e-01 -5.48698366e-01 -2.05940291e-01]]\n",
      "\n",
      " [[-4.88941550e-01 -7.29065657e-01  2.30154410e-01]\n",
      "  [-2.26591960e-01 -7.65634000e-01 -4.83671486e-01]\n",
      "  [ 1.72050342e-01 -7.98011959e-01 -6.21542275e-01]\n",
      "  [ 5.88699758e-01 -7.44752765e-01 -2.85393208e-01]\n",
      "  [ 5.19515812e-01 -6.62656844e-01 -1.27630830e-01]\n",
      "  [-5.33477962e-01 -6.88582718e-01 -5.56926310e-01]\n",
      "  [-9.74902391e-01 -5.58978796e-01  1.15383840e+00]\n",
      "  [-9.29632664e-01 -5.40628731e-01  2.15345526e+00]\n",
      "  [-2.01516077e-01 -7.50176013e-01 -3.02106477e-02]\n",
      "  [ 1.33775905e-01 -3.49688143e-01 -3.27230453e-01]\n",
      "  [ 2.13245839e-01 -2.69696683e-01 -2.24004075e-01]\n",
      "  [-6.42584682e-01 -8.02771449e-01  4.71137702e-01]\n",
      "  [ 2.12747119e-02 -2.15859488e-01  1.64566152e-02]\n",
      "  [-3.15083750e-02 -1.92288175e-01 -8.58734995e-02]\n",
      "  [-4.86320406e-01 -6.77495897e-01  3.41768920e-01]\n",
      "  [-2.06662640e-01 -7.20373154e-01 -2.53580421e-01]\n",
      "  [-2.37915054e-01 -7.37641394e-01 -4.51664239e-01]\n",
      "  [-5.16349316e-01 -5.65693676e-01 -1.51859209e-01]]\n",
      "\n",
      " [[-4.14678156e-01 -7.50361919e-01  2.88413405e-01]\n",
      "  [-1.43623337e-01 -7.79532135e-01 -5.05261064e-01]\n",
      "  [ 2.54256815e-01 -8.05145741e-01 -6.39940083e-01]\n",
      "  [ 6.65039241e-01 -7.62086511e-01 -2.72801191e-01]\n",
      "  [ 5.87792635e-01 -6.72988951e-01 -1.29924968e-01]\n",
      "  [-4.74783212e-01 -7.04705775e-01 -4.83499289e-01]\n",
      "  [-1.01651502e+00 -5.45994699e-01  1.44971836e+00]\n",
      "  [-9.33396935e-01 -5.63088894e-01  2.30311108e+00]\n",
      "  [-1.55056804e-01 -7.89591908e-01 -4.53791805e-02]\n",
      "  [ 1.05242550e-01 -3.54406863e-01 -2.90112495e-01]\n",
      "  [ 1.91419482e-01 -2.71835059e-01 -1.87717974e-01]\n",
      "  [-6.27381563e-01 -8.39689612e-01  5.40731490e-01]\n",
      "  [-1.77883729e-03 -2.25442424e-01  5.08858375e-02]\n",
      "  [-5.23957945e-02 -2.00914934e-01 -5.13169952e-02]\n",
      "  [-4.38554913e-01 -7.05771267e-01  4.18483078e-01]\n",
      "  [-1.19027987e-01 -7.45559394e-01 -2.99138248e-01]\n",
      "  [-1.79376721e-01 -7.52657115e-01 -4.68088090e-01]\n",
      "  [-4.69662547e-01 -5.87160766e-01 -1.16704732e-01]]\n",
      "\n",
      " [[-3.44175518e-01 -7.79843032e-01  3.20697904e-01]\n",
      "  [-7.85983801e-02 -7.92707860e-01 -5.22938311e-01]\n",
      "  [ 3.22078466e-01 -8.16876113e-01 -6.33909583e-01]\n",
      "  [ 7.21308589e-01 -7.82887042e-01 -2.49377608e-01]\n",
      "  [ 6.39649034e-01 -6.87512696e-01 -1.19776234e-01]\n",
      "  [-4.31474030e-01 -7.21910298e-01 -4.26460385e-01]\n",
      "  [-1.04462159e+00 -5.51679313e-01  1.67315316e+00]\n",
      "  [-9.34058905e-01 -5.99047363e-01  2.43851042e+00]\n",
      "  [-1.15991563e-01 -8.34316909e-01 -7.27600008e-02]\n",
      "  [ 7.76201189e-02 -3.58226508e-01 -2.54934877e-01]\n",
      "  [ 1.70733556e-01 -2.76346713e-01 -1.53712854e-01]\n",
      "  [-6.20468378e-01 -8.83901000e-01  5.84925294e-01]\n",
      "  [-2.27379091e-02 -2.36987904e-01  7.82925785e-02]\n",
      "  [-6.81718290e-02 -2.11929873e-01 -2.44380571e-02]\n",
      "  [-3.95313412e-01 -7.44003773e-01  4.74731505e-01]\n",
      "  [-4.84459624e-02 -7.67490566e-01 -3.44811767e-01]\n",
      "  [-1.33509442e-01 -7.70112693e-01 -4.79533136e-01]\n",
      "  [-4.29332644e-01 -6.10958993e-01 -1.01009563e-01]]\n",
      "\n",
      " [[-2.79568732e-01 -8.15280080e-01  3.21421623e-01]\n",
      "  [-3.23295929e-02 -8.05473030e-01 -5.31488538e-01]\n",
      "  [ 3.76538903e-01 -8.33198965e-01 -6.10061884e-01]\n",
      "  [ 7.59984016e-01 -8.06625128e-01 -2.14494362e-01]\n",
      "  [ 6.76032066e-01 -7.07588315e-01 -9.73443985e-02]\n",
      "  [-4.03793842e-01 -7.38537967e-01 -3.81966949e-01]\n",
      "  [-1.06645715e+00 -5.69712400e-01  1.84437799e+00]\n",
      "  [-9.29151297e-01 -6.43276811e-01  2.54758525e+00]\n",
      "  [-8.45072120e-02 -8.80191982e-01 -1.15012646e-01]\n",
      "  [ 5.44039719e-02 -3.60108525e-01 -2.20443353e-01]\n",
      "  [ 1.53023988e-01 -2.81183928e-01 -1.23813108e-01]\n",
      "  [-6.19308114e-01 -9.32840943e-01  5.99278331e-01]\n",
      "  [-4.03187089e-02 -2.47774437e-01  9.92662162e-02]\n",
      "  [-7.99101442e-02 -2.25209072e-01 -4.27133217e-03]\n",
      "  [-3.55145961e-01 -7.90336668e-01  5.08571446e-01]\n",
      "  [ 2.46706232e-03 -7.86255658e-01 -3.78346324e-01]\n",
      "  [-1.00528836e-01 -7.87964642e-01 -4.83215988e-01]\n",
      "  [-3.96225244e-01 -6.35965407e-01 -1.03319496e-01]]\n",
      "\n",
      " [[-2.22945824e-01 -8.50273848e-01  2.91695714e-01]\n",
      "  [-2.08568200e-03 -8.18050385e-01 -5.28680563e-01]\n",
      "  [ 4.20027524e-01 -8.53902400e-01 -5.71996391e-01]\n",
      "  [ 7.83465743e-01 -8.33423913e-01 -1.68239817e-01]\n",
      "  [ 6.99003816e-01 -7.31932819e-01 -6.37446195e-02]\n",
      "  [-3.89790744e-01 -7.53215253e-01 -3.43428612e-01]\n",
      "  [-1.08537591e+00 -5.93778670e-01  1.98088932e+00]\n",
      "  [-9.15754557e-01 -6.85503125e-01  2.61967349e+00]\n",
      "  [-6.00659475e-02 -9.25466418e-01 -1.72196001e-01]\n",
      "  [ 3.68975624e-02 -3.61665577e-01 -1.87926769e-01]\n",
      "  [ 1.38009071e-01 -2.85590261e-01 -9.68889445e-02]\n",
      "  [-6.18979812e-01 -9.81838882e-01  5.81875861e-01]\n",
      "  [-5.42666577e-02 -2.57806331e-01  1.17469892e-01]\n",
      "  [-8.94187540e-02 -2.38367155e-01  1.38441287e-02]\n",
      "  [-3.17769706e-01 -8.41786027e-01  5.17021537e-01]\n",
      "  [ 3.63579504e-02 -8.02887261e-01 -3.95312518e-01]\n",
      "  [-7.86101669e-02 -8.04506898e-01 -4.77231681e-01]\n",
      "  [-3.68975103e-01 -6.60821080e-01 -1.17120877e-01]]]\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the saved model weights\n",
    "checkpoint = torch.load('/home/maleen/research_data/Transformers/models/24_05_02_v2_best_model.pth', map_location=device)\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_joints = 18\n",
    "dropout_rate = 0.1\n",
    "input_length = 30\n",
    "predict_length = 1\n",
    "autoregressiveloops=10\n",
    "batch_size = 1\n",
    "\n",
    "# Initialize the models with the same configuration as during training\n",
    "embedding = SkeletalInputEmbedding(num_joints=num_joints, dof=3, embed_dim=embed_dim,device=device).to(device)\n",
    "#t_embedding = TargetEmbedding(num_joints=num_joints, dof=3, embed_dim=embed_dim,device=device).to(device)\n",
    "encoder = TransformerEncoder(embed_dim, num_heads, num_layers, dropout_rate).to(device)\n",
    "decoder = TransformerDecoder(embed_dim, num_heads, num_layers, num_joints, dropout_rate).to(device)\n",
    "\n",
    "# Load state dicts\n",
    "embedding.load_state_dict(checkpoint['embedding_state_dict'])\n",
    "#t_embedding.load_state_dict(checkpoint['t_embedding_state_dict'])\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "embedding.eval()\n",
    "#t_embedding.eval()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "\n",
    "X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask = generate_sequences(norm_pos, norm_vel, norm_acc, masks, input_length, predict_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_pos_tensor = torch.tensor(X_pos, dtype=torch.float32)\n",
    "X_vel_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "X_acc_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "X_mask_tensor = torch.tensor(X_mask, dtype=torch.bool)\n",
    "\n",
    "Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "Y_vel_tensor = torch.tensor(Y_vel, dtype=torch.float32)\n",
    "Y_acc_tensor = torch.tensor(Y_acc, dtype=torch.float32)\n",
    "Y_mask_tensor = torch.tensor(Y_mask, dtype=torch.bool)\n",
    "\n",
    "# Create the DataLoader for inference data\n",
    "dataset = TensorDataset(X_pos_tensor, X_vel_tensor, X_acc_tensor, X_mask_tensor, Y_pos_tensor, Y_mask_tensor)\n",
    "inference_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prepare for autoregressive decoding\n",
    "predicted_positions = []\n",
    "\n",
    "criterion = MaskedMSELoss()\n",
    "\n",
    "# Perform inference across all batches\n",
    "for batch in inference_loader:\n",
    "    X_pos_batch, X_vel_batch, X_acc_batch, X_mask_batch, Y_pos_batch, Y_mask_batch = [b.to(device) for b in batch]\n",
    "\n",
    "    # Encoder pass\n",
    "    src_key_padding_mask = ~X_mask_batch.view(batch_size, input_length * num_joints)\n",
    "    input_embeddings = embedding(X_pos_batch, X_mask_batch)\n",
    "    memory = encoder(input_embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "    \n",
    "    # Initialize the start token for decoding\n",
    "    current_pos = X_pos_batch[:, -1:, :, :]\n",
    "    current_mask = X_mask_batch[:,-1:,:]\n",
    "\n",
    "    for i in range(autoregressiveloops):\n",
    "        # Embed the current position\n",
    "        Y_expected= Y_pos_batch[:,i:i+1,:,:]\n",
    "        Y_mask_expected = Y_mask_batch[:,i:i+1,:]\n",
    "\n",
    "        # # #Running whole model\n",
    "        # if i > 0:\n",
    "        #     X_mask_batch_ar = torch.cat([X_mask_batch[:, 1:, :], current_mask], dim=1)\n",
    "        #     X_pos_batch_ar = torch.cat([X_pos_batch[:, 1:, :, :], current_pos], dim=1)\n",
    "    \n",
    "        #     src_key_padding_mask = ~X_mask_batch_ar.view(batch_size, input_length * num_joints)\n",
    "        #     input_embeddings = embedding(X_pos_batch_ar, X_mask_batch_ar)\n",
    "        #     memory = encoder(input_embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "        # ##\n",
    "        \n",
    "        current_embeddings = embedding(current_pos, current_mask)\n",
    "        \n",
    "        # Decoder pass\n",
    "        output = decoder(current_embeddings, memory, tgt_key_padding_mask=None, memory_key_padding_mask=src_key_padding_mask)\n",
    "        print(output.shape)\n",
    "    \n",
    "        # Update current_pos for the next prediction\n",
    "        current_pos = output[:, :, :, :].detach()  # only take the last timestep\n",
    "    \n",
    "        predicted_positions.append(current_pos.squeeze().cpu().numpy())\n",
    "    \n",
    "        Xmask_expanded = current_mask.unsqueeze(-1).expand_as(output)\n",
    "        Xmask_expanded = Xmask_expanded.where(~torch.isnan(output), torch.zeros_like(Xmask_expanded))\n",
    "        output = output.where(~torch.isnan(output), torch.zeros_like(output))\n",
    "        # masked_output = output * Xmask\n",
    "    \n",
    "        Ymask_expanded = Y_mask_expected.unsqueeze(-1).expand_as(Y_expected)\n",
    "        Ymask_expanded = Ymask_expanded.where(~torch.isnan(Y_expected), torch.zeros_like(Ymask_expanded))\n",
    "        Y_expected = Y_expected.where(~torch.isnan(Y_pos_batch), torch.zeros_like(Y_expected))\n",
    "        # masked_y_pos = Y_pos_batch * Ymask\n",
    "    \n",
    "        # Compute loss\n",
    "        \n",
    "        loss = criterion(output, Y_expected, Xmask_expanded, Ymask_expanded)\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "    break\n",
    "# Convert the list of predicted positions to a more manageable form, e.g., a NumPy array\n",
    "predicted_positions = np.array(predicted_positions)\n",
    "\n",
    "print(\"Predicted Positions:\", predicted_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_normalization(normalized_data, medians_per_joint_axis, iqrs_per_joint_axis, masks):\n",
    "    original_data = np.empty_like(normalized_data)  # Initialize an array to hold the original data\n",
    "\n",
    "    # Iterate over each joint and each axis\n",
    "    for joint in range(normalized_data.shape[0]):\n",
    "        for axis in range(normalized_data.shape[1]):\n",
    "            # Retrieve the median and IQR for this joint and axis\n",
    "            median = medians_per_joint_axis[joint, axis]\n",
    "            iqr = iqrs_per_joint_axis[joint, axis]\n",
    "\n",
    "            # Retrieve the normalized values for this joint and axis\n",
    "            normalized_values = normalized_data[ joint, axis]\n",
    "\n",
    "            # Calculate the original values based on the normalization formula\n",
    "            original_values = (normalized_values * iqr) + median\n",
    "\n",
    "            # Apply the mask to restore original data only where it was initially present\n",
    "            mask_for_joint = masks[:, joint]\n",
    "            original_data[ joint, axis] = np.where(mask_for_joint == 1, original_values, np.nan)\n",
    "\n",
    "    return original_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc36b5b523634fd6bb507ec8ff6ca927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Container(figure=Figure(box_center=[0.5, 0.5, 0.5], box_size=[1.0, 1.0, 1.0], camera=PerspectiveCamera(fov=45.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_connections = [\n",
    "    (16, 14), (14, 0),               # Right Head\n",
    "    (17, 15), (15, 0),               # Left Head\n",
    "    (0, 1),                          # Neck\n",
    "    (1, 2), (2, 3), (3, 4),          # Right arm\n",
    "    (1, 5), (5, 6), (6, 7),          # Left arm\n",
    "    (1, 8), (8, 9), (9, 10),         # Right leg\n",
    "    (1, 11), (11, 12), (12, 13),     # Left leg\n",
    "    (8, 11)                          # Between hips\n",
    "]\n",
    "\n",
    "pred=0\n",
    "# First dataset processing (already provided code)\n",
    "mask = np.array(X_mask_batch[0][pred].cpu())\n",
    "mask_expanded = np.tile(mask[:, np.newaxis], (1, predicted_positions[pred].shape[1]))\n",
    "mask_ones = np.ones((1, 18))\n",
    "reversed_data = reverse_normalization(predicted_positions[pred], medians_pos, iqrs_pos, mask_ones)\n",
    "masked_reversed_data = np.where(mask_expanded, reversed_data, np.nan)\n",
    "data = masked_reversed_data\n",
    "valid_keypoints = ~np.isnan(data[:18, :]).any(axis=1)\n",
    "filtered_data = data[:18][valid_keypoints]\n",
    "\n",
    "# Create mapping from old indices to new indices after NaN removal\n",
    "index_mapping = {old_index: new_index for new_index, old_index in enumerate(np.where(valid_keypoints)[0])}\n",
    "# Create new connections for the first dataset\n",
    "new_connections = [(index_mapping[start], index_mapping[end])\n",
    "                   for start, end in original_connections\n",
    "                   if start in index_mapping and end in index_mapping]\n",
    "\n",
    "# Second dataset processing\n",
    "mask_y = Y_mask[0][pred] # Assuming you need a similar mask or adjust accordingly\n",
    "mask_expanded_y = np.tile(mask_y[:, np.newaxis], (1, Y_pos[0][pred].shape[1]))\n",
    "reversed_data_y = reverse_normalization(Y_pos[0][pred], medians_pos, iqrs_pos, mask_ones)\n",
    "masked_reversed_data_y = np.where(mask_expanded_y, reversed_data_y, np.nan)\n",
    "data_y = masked_reversed_data_y\n",
    "valid_keypoints_y = ~np.isnan(data_y[:18, :]).any(axis=1)\n",
    "filtered_data_y = data_y[:18][valid_keypoints_y]\n",
    "\n",
    "# Create mapping from old indices to new indices for the second dataset\n",
    "index_mapping_y = {old_index: new_index for new_index, old_index in enumerate(np.where(valid_keypoints_y)[0])}\n",
    "# Create new connections for the second dataset\n",
    "new_connections_y = [(index_mapping_y[start], index_mapping_y[end])\n",
    "                     for start, end in original_connections\n",
    "                     if start in index_mapping_y and end in index_mapping_y]\n",
    "\n",
    "# Plot configuration\n",
    "ipv.figure()\n",
    "\n",
    "# Plot first dataset\n",
    "scatter = ipv.scatter(filtered_data[:, 0], filtered_data[:, 1], filtered_data[:, 2], color='blue', marker='sphere', size=2)\n",
    "for start, end in new_connections:\n",
    "    ipv.plot(filtered_data[[start, end], 0], filtered_data[[start, end], 1], filtered_data[[start, end], 2], color='red')\n",
    "\n",
    "# Plot second dataset\n",
    "scatter_y = ipv.scatter(filtered_data_y[:, 0], filtered_data_y[:, 1], filtered_data_y[:, 2], color='green', marker='sphere', size=2)\n",
    "for start, end in new_connections_y:\n",
    "    ipv.plot(filtered_data_y[[start, end], 0], filtered_data_y[[start, end], 1], filtered_data_y[[start, end], 2], color='lime')\n",
    "\n",
    "ipv.view(azimuth=0, elevation=-90)\n",
    "ipv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47483402, -0.71264905,  0.28098685],\n",
       "       [-0.21476673, -0.7537337 , -0.4730864 ],\n",
       "       [ 0.19638036, -0.786061  , -0.6226275 ],\n",
       "       [ 0.6040235 , -0.73149794, -0.2870397 ],\n",
       "       [ 0.5357953 , -0.64622986, -0.13925472],\n",
       "       [-0.5236368 , -0.6763079 , -0.54583687],\n",
       "       [-0.97072184, -0.5446813 ,  1.184758  ],\n",
       "       [-0.9134797 , -0.5286785 ,  2.1569881 ],\n",
       "       [-0.19354278, -0.73181623,  0.00370101],\n",
       "       [ 0.13781427, -0.34933838, -0.32133228],\n",
       "       [ 0.21963577, -0.2693188 , -0.22174908],\n",
       "       [-0.6293069 , -0.78906006,  0.47372437],\n",
       "       [ 0.03185884, -0.2119381 ,  0.01111295],\n",
       "       [-0.02125384, -0.19045211, -0.09410442],\n",
       "       [-0.47380498, -0.6696339 ,  0.379654  ],\n",
       "       [-0.18705656, -0.71240383, -0.23537967],\n",
       "       [-0.22284709, -0.72880656, -0.44298863],\n",
       "       [-0.50438493, -0.55365276, -0.12553309]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_positions[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False,  True, False, False,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

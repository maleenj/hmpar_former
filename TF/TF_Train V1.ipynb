{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "033b1eb5-8e73-46fa-97e8-30208fb91eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from embedding_layers import SkeletalInputEmbedding\n",
    "from encoder_layers import TransformerEncoder\n",
    "from decoder_layers import TransformerDecoder\n",
    "import TF_helper_functions as hf\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bb28bfa-65b6-4023-a035-35036b5aa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath='/home/maleen/research_data/Transformers/datasets/training/'\n",
    "# Base path and file information\n",
    "base_name='24_07_25_training_norm'\n",
    "\n",
    "filename=datapath+base_name+'.pkl'\n",
    "\n",
    "def load_results_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def process_all_datasets(results, input_length=60, predict_length=60):\n",
    "    all_X_pos, all_X_vel, all_X_acc = [], [], []\n",
    "    all_Y_pos, all_Y_vel, all_Y_acc = [], [], []\n",
    "    discarded_frames = {}\n",
    "\n",
    "    for i in range(1, 7):  # Assuming you have 6 datasets\n",
    "        dataset_key = f'dataset{i}'\n",
    "        norm_pos = results[f'{dataset_key}_normpos']\n",
    "        norm_vel = results[f'{dataset_key}_normvel']\n",
    "        norm_acc = results[f'{dataset_key}_normacc']\n",
    "\n",
    "        # Generate sequences for this dataset\n",
    "        X_pos, X_vel, X_acc, Y_pos, Y_vel, Y_acc = hf.generate_sequences(norm_pos, norm_vel, norm_acc, input_length, predict_length)\n",
    "        \n",
    "        all_X_pos.append(X_pos)\n",
    "        all_X_vel.append(X_vel)\n",
    "        all_X_acc.append(X_acc)\n",
    "        all_Y_pos.append(Y_pos)\n",
    "        all_Y_vel.append(Y_vel)\n",
    "        all_Y_acc.append(Y_acc)\n",
    "\n",
    "        # Calculate discarded frames\n",
    "        total_frames = norm_pos.shape[0]\n",
    "        used_frames = X_pos.shape[0] + input_length + predict_length - 1\n",
    "        discarded = total_frames - used_frames\n",
    "        discarded_frames[dataset_key] = discarded\n",
    "\n",
    "    # Combine sequences from all datasets\n",
    "    combined_X_pos = np.concatenate(all_X_pos)\n",
    "    combined_X_vel = np.concatenate(all_X_vel)\n",
    "    combined_X_acc = np.concatenate(all_X_acc)\n",
    "    combined_Y_pos = np.concatenate(all_Y_pos)\n",
    "    combined_Y_vel = np.concatenate(all_Y_vel)\n",
    "    combined_Y_acc = np.concatenate(all_Y_acc)\n",
    "\n",
    "    return (combined_X_pos, combined_X_vel, combined_X_acc, \n",
    "            combined_Y_pos, combined_Y_vel, combined_Y_acc, \n",
    "            discarded_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b76f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sequences shapes:\n",
      "X_pos shape: (7667, 30, 6, 3)\n",
      "X_vel shape: (7667, 30, 6, 3)\n",
      "X_acc shape: (7667, 30, 6, 3)\n",
      "Y_pos shape: (7667, 2, 6, 3)\n",
      "Y_vel shape: (7667, 2, 6, 3)\n",
      "Y_acc shape: (7667, 2, 6, 3)\n",
      "\n",
      "Discarded frames per dataset:\n",
      "dataset1: 0 frames\n",
      "dataset2: 0 frames\n",
      "dataset3: 0 frames\n",
      "dataset4: 0 frames\n",
      "dataset5: 0 frames\n",
      "dataset6: 0 frames\n"
     ]
    }
   ],
   "source": [
    "input_length = 30\n",
    "predict_length = 2\n",
    "datasetnum=6\n",
    "\n",
    "# Load the results\n",
    "results = load_results_from_pickle(filename)\n",
    "\n",
    "# Process all datasets and get combined sequences\n",
    "(combined_X_pos, combined_X_vel, combined_X_acc, \n",
    " combined_Y_pos, combined_Y_vel, combined_Y_acc, \n",
    " discarded_frames) = process_all_datasets(results, input_length, predict_length)\n",
    "\n",
    "print(\"Combined sequences shapes:\")\n",
    "print(f\"X_pos shape: {combined_X_pos.shape}\")\n",
    "print(f\"X_vel shape: {combined_X_vel.shape}\")\n",
    "print(f\"X_acc shape: {combined_X_acc.shape}\")\n",
    "print(f\"Y_pos shape: {combined_Y_pos.shape}\")\n",
    "print(f\"Y_vel shape: {combined_Y_vel.shape}\")\n",
    "print(f\"Y_acc shape: {combined_Y_acc.shape}\")\n",
    "\n",
    "print(\"\\nDiscarded frames per dataset:\")\n",
    "for dataset, frames in discarded_frames.items():\n",
    "    print(f\"{dataset}: {frames} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8abd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a82bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

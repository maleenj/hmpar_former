{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ipyvolume as ipv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileroot='2024_03_08_TRF_Maleen/'\n",
    "filename='2024_03_08_TRF_Maleen_01'\n",
    "\n",
    "datapath='/home/maleen/research_data/Transformers/datasets/training/'\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "df1 = pd.read_pickle(datapath + filename + '.pkl')\n",
    "\n",
    "arrays = [np.array(item) for item in df1['Skeleton_3D']]\n",
    "timestamps = [np.array(item) for item in df1['Skeleton_Timestamp']]\n",
    "\n",
    "# Convert datetime to seconds from start\n",
    "timestamps = (timestamps - timestamps[0])\n",
    "\n",
    "# Stack these arrays along a new axis to create a 3D NumPy array\n",
    "# Each \"slice\" of this 3D array represents one frame of keypoints\n",
    "skeleton_3d_frames = np.stack(arrays, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated skeleton_3d_frames shape: (1020, 14, 3)\n",
      "Updated connections: [(12, 10), (10, 0), (13, 11), (11, 0), (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8), (1, 9), (8, 9)]\n"
     ]
    }
   ],
   "source": [
    "# Assume skeleton_3d_frames is already defined\n",
    "# skeleton_3d_frames shape is (1020, 18, 3)\n",
    "\n",
    "# Indices of the joints to be removed\n",
    "joints_to_remove = [9, 10, 12, 13]\n",
    "\n",
    "# Create a mask to keep the joints that are not to be removed\n",
    "mask = np.ones(skeleton_3d_frames.shape[1], dtype=bool)\n",
    "mask[joints_to_remove] = False\n",
    "\n",
    "# Update skeleton_3d_frames to remove the specified joints\n",
    "skeleton_3d_frames_updated = skeleton_3d_frames[:, mask, :]\n",
    "\n",
    "# Original connections\n",
    "original_connections = [\n",
    "    (16, 14), (14, 0),               # Right Head\n",
    "    (17, 15), (15, 0),               # Left Head\n",
    "    (0, 1),                          # Neck\n",
    "    (1, 2), (2, 3), (3, 4),          # Right arm\n",
    "    (1, 5), (5, 6), (6, 7),          # Left arm\n",
    "    (1, 8), (8, 9), (9, 10),         # Right leg\n",
    "    (1, 11), (11, 12), (12, 13),     # Left leg\n",
    "    (8, 11)                          # Between hips\n",
    "]\n",
    "\n",
    "# Mapping of old joint indices to new indices\n",
    "index_mapping = {}\n",
    "new_index = 0\n",
    "for old_index in range(skeleton_3d_frames.shape[1]):\n",
    "    if old_index not in joints_to_remove:\n",
    "        index_mapping[old_index] = new_index\n",
    "        new_index += 1\n",
    "\n",
    "# Update the connections to reflect the new indices\n",
    "new_connections = []\n",
    "for connection in original_connections:\n",
    "    if connection[0] in index_mapping and connection[1] in index_mapping:\n",
    "        new_connections.append((index_mapping[connection[0]], index_mapping[connection[1]]))\n",
    "\n",
    "# Print the updated connections\n",
    "print(\"Updated skeleton_3d_frames shape:\", skeleton_3d_frames_updated.shape)\n",
    "print(\"Updated connections:\", new_connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with NaN values: [   0    1   49   89   90   91   92   93   94   95   96   97   98   99\n",
      "  100  143  144  145  208  231  232  233  234  239  289  296  325  326\n",
      "  327  328  329  330  331  332  333  334  337  338  345  346  347  348\n",
      "  349  350  351  352  353  354  355  356  357  358  359  360  361  362\n",
      "  363  364  365  366  367  368  369  370  371  372  373  374  375  376\n",
      "  377  378  379  380  381  382  383  384  385  386  387  388  389  390\n",
      "  391  392  393  395  422  423  424  425  426  549  550  551  552  553\n",
      "  554  555  556  557  558  559  560  561  562  563  564  565  566  567\n",
      "  568  569  570  571  572  573  578  582  669  670  671  672  673  674\n",
      "  675  676  677  678  679  680  681  685  686  687  688  690  694  695\n",
      "  696  697  738  739  740  741  742  743  744  745  746  747  748  749\n",
      "  750  751  752  753  754  755  860  861  862  863  864  865  866  867\n",
      "  868  872  938 1018 1019]\n",
      "Joints with NaN values: {2, 4, 6, 7, 13}\n",
      "Original shape: (1020, 14, 3)\n",
      "New shape after removing frames with NaNs: (847, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any NaN values in skeleton_3d_frames_updated\n",
    "nan_indices = np.isnan(skeleton_3d_frames_updated)\n",
    "\n",
    "# Find the indices of frames and joints that have NaN values\n",
    "frames_with_nans, joints_with_nans, _ = np.where(nan_indices)\n",
    "\n",
    "# Create a set of unique joints that have NaN values\n",
    "unique_joints_with_nans = set(joints_with_nans)\n",
    "\n",
    "print(\"Frames with NaN values:\", np.unique(frames_with_nans))\n",
    "print(\"Joints with NaN values:\", unique_joints_with_nans)\n",
    "\n",
    "# Find the indices of frames that have NaN values\n",
    "frames_with_nans = np.unique(np.where(nan_indices)[0])\n",
    "\n",
    "# Remove the frames with NaN values\n",
    "skeleton_3d_frames_cleaned = np.delete(skeleton_3d_frames_updated, frames_with_nans, axis=0)\n",
    "timestamps_cleaned=np.delete(timestamps, frames_with_nans, axis=0)\n",
    "\n",
    "print(\"Original shape:\", skeleton_3d_frames_updated.shape)\n",
    "print(\"New shape after removing frames with NaNs:\", skeleton_3d_frames_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the differences in position and time\n",
    "position_diff = np.diff(skeleton_3d_frames_cleaned, axis=0)\n",
    "time_diff = np.diff(timestamps_cleaned)\n",
    "\n",
    "# Ensure that time_diff is of shape (n,1,1) so that it broadcasts correctly when dividing\n",
    "time_diff = time_diff[:, np.newaxis, np.newaxis]\n",
    "\n",
    "\n",
    "# Now we calculate velocity, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_vel = position_diff / time_diff\n",
    "\n",
    "# Calculate the differences in velocity\n",
    "velocity_diff = np.diff(skel_vel, axis=0)\n",
    "\n",
    "# Now we calculate acceleration, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_acc = velocity_diff / time_diff[:-1, :, :]  # Use time_diff with one less time dimension\n",
    "\n",
    "skel_pos= skeleton_3d_frames_cleaned[2:]\n",
    "skel_vel = skel_vel[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(845, 14, 3)\n",
      "(845, 14, 3)\n",
      "(845, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "# Now, slice skeleton_3d_frames and skel_vel to match the dimensions of skell_acc\n",
    "print(skel_pos.shape)\n",
    "print(skel_vel.shape)\n",
    "print(skel_acc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to hold the normalized data, medians, and IQRs\n",
    "norm_pos = np.empty_like(skel_pos)\n",
    "medians_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "iqrs_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "\n",
    "norm_vel= np.empty_like(skel_vel)\n",
    "medians_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "iqrs_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "\n",
    "norm_acc = np.empty_like(skel_acc)\n",
    "medians_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "iqrs_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "\n",
    "def robust_normalize_data_with_clipping(data, medians_per_joint_axis, iqrs_per_joint_axis, normalized_data, clipping_percentiles=(1, 99)):\n",
    "    for joint in range(data.shape[1]):  # For each joint\n",
    "        for axis in range(data.shape[2]):  # For each axis (x, y, z)\n",
    "            joint_axis_data = data[:, joint, axis]\n",
    "\n",
    "            # Determine clipping thresholds based on percentiles\n",
    "            lower_threshold, upper_threshold = np.percentile(joint_axis_data, clipping_percentiles)\n",
    "\n",
    "            # Clip the data based on thresholds\n",
    "            clipped_values = np.clip(joint_axis_data, lower_threshold, upper_threshold)\n",
    "\n",
    "            # Calculate median and IQR for clipped data\n",
    "            median = np.median(clipped_values)\n",
    "            q75, q25 = np.percentile(clipped_values, [75, 25])\n",
    "            iqr = q75 - q25\n",
    "\n",
    "            # Store the calculated medians and IQRs\n",
    "            medians_per_joint_axis[joint, axis] = median\n",
    "            iqrs_per_joint_axis[joint, axis] = iqr\n",
    "\n",
    "            # Normalize the clipped data, avoiding division by zero\n",
    "            if iqr > 0:\n",
    "                normalized_values = (clipped_values - median) / iqr\n",
    "            else:\n",
    "                normalized_values = clipped_values  # Keep original values if IQR is 0\n",
    "\n",
    "            # Store the normalized values\n",
    "            normalized_data[:, joint, axis] = normalized_values\n",
    "\n",
    "    return normalized_data, medians_per_joint_axis, iqrs_per_joint_axis\n",
    "# Example usage with your data\n",
    "# Note: masks, skel_pos, skel_vel, skel_acc should be defined in your context\n",
    "\n",
    "norm_pos, medians_pos, iqrs_pos = robust_normalize_data_with_clipping(skel_pos, medians_per_joint_axis_skel_pos, iqrs_per_joint_axis_skel_pos, norm_pos)\n",
    "norm_vel, medians_vel, iqrs_vel = robust_normalize_data_with_clipping(skel_vel, medians_per_joint_axis_vel, iqrs_per_joint_axis_vel, norm_vel)\n",
    "norm_acc, medians_acc, iqrs_acc = robust_normalize_data_with_clipping(skel_acc, medians_per_joint_axis_acc, iqrs_per_joint_axis_acc, norm_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of normalized data: 0.0\n",
      "Data points within [-1, 1] (IQR): 845 out of 845\n",
      "Original Data points within [-1, 1] (IQR): 98 out of 845\n"
     ]
    }
   ],
   "source": [
    "data=skel_pos\n",
    "norm_data=norm_pos\n",
    "# Plot the original and normalized data for a specific joint and axis\n",
    "joint, axis = 0, 0  # Change as needed\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[:, joint, axis], bins=20, alpha=0.7, label='Original')\n",
    "plt.title(\"Original Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(norm_data[:, joint, axis], bins=20, alpha=0.7, label='Normalized')\n",
    "plt.title(\"Normalized Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check the median and range of the normalized data\n",
    "normalized_median = np.nanmedian(norm_data[:, joint, axis])\n",
    "print(\"Median of normalized data:\", normalized_median)\n",
    "\n",
    "within_iqr = ((norm_data[:, joint, axis] > -2) & (norm_data[:, joint, axis] < 2)).sum()\n",
    "print(f\"Data points within [-1, 1] (IQR): {within_iqr} out of {norm_data.shape[0]}\")\n",
    "\n",
    "within_iqr2 = ((data[:, joint, axis] > -1) & (data[:, joint, axis] < 1)).sum()\n",
    "print(f\"Original Data points within [-1, 1] (IQR): {within_iqr2} out of {norm_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(845, 14, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(norm_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletalInputEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128, device='cuda'):\n",
    "        super(SkeletalInputEmbedding, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Linear layer to project input_dim to embed_dim\n",
    "        self.input_embed = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, joint_positions):\n",
    "        # joint_positions shape: (batch_size, seq_len, num_joints, dof)\n",
    "        batch_size, seq_len, num_joints, dof = joint_positions.size()\n",
    "        input_dim = num_joints * dof  # Total input dimension\n",
    "\n",
    "        # Reshape to (batch_size * seq_len, num_joints * dof)\n",
    "        joint_positions = joint_positions.view(batch_size * seq_len, input_dim)\n",
    "\n",
    "        # Apply the linear layer to project to embed_dim\n",
    "        embeddings = self.input_embed(joint_positions)\n",
    "\n",
    "        # Reshape back to (batch_size, seq_len, embed_dim)\n",
    "        embeddings = embeddings.view(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        # Calculate positional encoding\n",
    "        positional_encoding = self.get_sinusoidal_encoding(seq_len, self.embed_dim).to(self.device)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0).expand(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        # Add positional encoding to the embeddings\n",
    "        embeddings += positional_encoding\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def get_sinusoidal_encoding(self, total_len, embed_dim):\n",
    "        position = torch.arange(0, total_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pe = torch.zeros(total_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEmbedding(nn.Module):\n",
    "    def __init__(self, num_joints=18, dof=3, embed_dim=128, device='gpu'):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.dof = dof\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device  # Store the device\n",
    "\n",
    "        self.joint_embed = nn.Linear(dof, embed_dim)\n",
    "        # self.vel_embed = nn.Linear(dof, embed_dim)\n",
    "        # self.acc_embed = nn.Linear(dof, embed_dim)\n",
    "\n",
    "    #def forward(self, joint_positions, velocities, accelerations, mask=None):\n",
    "    def forward(self, joint_positions, mask=None):\n",
    "        # Replace NaNs in the input data\n",
    "        joint_positions = torch.nan_to_num(joint_positions)\n",
    "        # velocities = torch.nan_to_num(velocities)\n",
    "        # accelerations = torch.nan_to_num(accelerations)\n",
    "\n",
    "\n",
    "        # Embedding and combining the embeddings\n",
    "        joint_embeddings = self.joint_embed(joint_positions)\n",
    "        # vel_embeddings = self.vel_embed(velocities)\n",
    "        # acc_embeddings = self.acc_embed(accelerations)\n",
    "        combined_embeddings = joint_embeddings #+ vel_embeddings + acc_embeddings\n",
    "\n",
    "        # # Apply mask after NaN replacement\n",
    "        # if mask is not None:\n",
    "        #     mask = mask.unsqueeze(-1)  # Add a dimension for the features\n",
    "        #     combined_embeddings = combined_embeddings * mask\n",
    "\n",
    "        # Calculate positional encoding dynamically\n",
    "        seq_len = joint_positions.size(1)  # Assuming joint_positions is [seq_len, num_joints, dof]\n",
    "        positional_encoding = self.get_sinusoidal_encoding(seq_len, self.num_joints*self.embed_dim)\n",
    "        # print(\"Device of input embeddings:\", combined_embeddings.device)\n",
    "        # print(\"Device of positional encodings:\", positional_encoding.device)\n",
    "\n",
    "        positional_encoding = positional_encoding.view(seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        combined_embeddings += positional_encoding.unsqueeze(0)  # Unsqueeze to add batch dimension for broadcasting\n",
    "        combined_embeddings = combined_embeddings.view(-1, seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        return combined_embeddings\n",
    "\n",
    "    def get_sinusoidal_encoding(self, total_len, embed_dim):\n",
    "        # Make sure the position tensor is created on the right device\n",
    "        position = torch.arange(0, total_len, dtype=torch.float, device=self.device).unsqueeze(1)\n",
    "    \n",
    "        # Calculate the division term for sinusoidal encoding\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim)).to(self.device)\n",
    "        div_term = div_term.unsqueeze(0)  # Reshape for broadcasting\n",
    "    \n",
    "        # Create the positional encoding matrix\n",
    "        pe = torch.zeros(total_len, embed_dim, device=self.device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "        return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        src: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        src_mask: None or Tensor for masking in multi-head attention (not used in this example)\n",
    "        src_key_padding_mask: Tensor of shape (batch_size, seq_len) indicating which elements are padded\n",
    "        \"\"\"\n",
    "        # Applying Transformer Encoder\n",
    "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, num_joints, dropout_rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_joints = num_joints\n",
    "\n",
    "        # Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.embed_dim,  # Keeping d_model consistent with embed_dim\n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        # Output layer to convert decoder output to joint position dimension\n",
    "        self.output_layer = nn.Linear(self.embed_dim, self.num_joints * 3)  # Assuming output per joint is a 3D position\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: Tensor of shape (batch_size, output_seq_len, embed_dim), initially could be start token or zero vectors\n",
    "        memory: Tensor of shape (batch_size, input_seq_len, embed_dim), output from the Transformer encoder\n",
    "        tgt_mask: Mask to ensure the decoder's predictions are based only on past positions\n",
    "        memory_mask: Optional, to mask encoder outputs if necessary\n",
    "        tgt_key_padding_mask: Tensor of shape (batch_size, output_seq_len) for masking target sequences\n",
    "        memory_key_padding_mask: Tensor of shape (batch_size, input_seq_len) for masking memory sequences\n",
    "        \"\"\"\n",
    "\n",
    "        # Transformer Decoder\n",
    "        output = self.transformer_decoder(\n",
    "            tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "        # Project to joint position dimensions\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        # Reshape to (batch_size, seq_len, num_joints, 3)\n",
    "        output = output.view(output.size(0), output.size(1), self.num_joints, 3)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(norm_pos, norm_vel, norm_acc, input_length=60, predict_length=60):\n",
    "    num_frames = norm_pos.shape[0]\n",
    "    num_joints = norm_pos.shape[1]\n",
    "\n",
    "    # Calculate the total number of sequences we can create\n",
    "    num_sequences = num_frames - input_length - predict_length + 1\n",
    "\n",
    "    # Initialize arrays to store the input and target sequences\n",
    "    X_pos = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_vel = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_acc = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    Y_pos = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_vel = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_acc = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    # X_mask = np.zeros((num_sequences, input_length, num_joints))\n",
    "    # Y_mask = np.zeros((num_sequences, predict_length, num_joints))\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(num_sequences):\n",
    "        X_pos[i] = norm_pos[i:i + input_length]\n",
    "        X_vel[i] = norm_vel[i:i + input_length]\n",
    "        X_acc[i] = norm_acc[i:i + input_length]\n",
    "        Y_pos[i] = norm_pos[i + input_length:i + input_length + predict_length]\n",
    "        Y_vel[i] = norm_vel[i + input_length:i + input_length + predict_length]\n",
    "        Y_acc[i] = norm_acc[i + input_length:i + input_length + predict_length]\n",
    "        # X_mask[i] = mask[i:i + input_length]\n",
    "        # Y_mask[i] = mask[i + input_length:i + input_length + predict_length]\n",
    "\n",
    "    return X_pos, X_vel, X_acc, Y_pos, Y_vel, Y_acc\n",
    "\n",
    "\n",
    "def create_shifted_mask(seq_length, num_joints):\n",
    "    # seq_length is the number of time steps\n",
    "    # num_joints is the number of joints per time step\n",
    "    total_length = seq_length * num_joints\n",
    "    mask = torch.ones((total_length, total_length), dtype=torch.float32) * float('-inf')  # Start with everything masked\n",
    "    for i in range(seq_length):\n",
    "        for j in range(i + 1):  # Allow visibility up to and including the current time step\n",
    "            start_row = i * num_joints\n",
    "            end_row = start_row + num_joints\n",
    "            start_col = j * num_joints\n",
    "            end_col = start_col + num_joints\n",
    "            mask[start_row:end_row, start_col:end_col] = 0.0  # Unmask the allowed region\n",
    "\n",
    "    return mask\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Compute the squared differences\n",
    "        squared_diff = (output - target) ** 2\n",
    "\n",
    "        # Calculate the mean of the squared differences\n",
    "        loss = squared_diff.mean()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.1099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.1282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.1546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.1988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.2477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.2363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.2481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.2702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.2914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.3251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.3210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.3325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.3750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.4216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.4544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.4927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.4989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.5375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.5019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.5201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.5547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.6197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.6640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1, 14, 3])\n",
      "tensor(0.7098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Predicted Positions: [[[-0.8355402  -0.75666267 -0.44037154]\n",
      "  [-0.7674685  -0.69441855 -0.5615143 ]\n",
      "  [-0.2578715  -0.718956   -0.04041005]\n",
      "  ...\n",
      "  [-0.8450513  -0.7080816  -0.5323335 ]\n",
      "  [-0.61332303 -0.753781   -0.3487503 ]\n",
      "  [-0.81558734 -0.698449   -0.84762776]]\n",
      "\n",
      " [[-0.662664   -0.7405122  -0.3640838 ]\n",
      "  [-0.62506664 -0.68831104 -0.466452  ]\n",
      "  [-0.08213588 -0.7531277  -0.20078951]\n",
      "  ...\n",
      "  [-0.70450616 -0.7069415  -0.4682665 ]\n",
      "  [-0.5563997  -0.72209    -0.57549375]\n",
      "  [-0.6271151  -0.7175642  -0.58426464]]\n",
      "\n",
      " [[-0.4915939  -0.71288157 -0.27795526]\n",
      "  [-0.50422364 -0.6751401  -0.34154463]\n",
      "  [ 0.09749032 -0.73585165 -0.22520423]\n",
      "  ...\n",
      "  [-0.55198383 -0.67882216 -0.40349162]\n",
      "  [-0.52091604 -0.6767967  -0.766911  ]\n",
      "  [-0.46980792 -0.6993919  -0.41388035]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.3114191  -0.7318919  -0.2518461 ]\n",
      "  [ 0.3518436  -0.72969145 -0.12358464]\n",
      "  [ 1.1181253  -0.80142254 -0.12711857]\n",
      "  ...\n",
      "  [ 0.27382055 -0.66530573 -0.34963205]\n",
      "  [ 0.20536532 -0.7574151  -0.9210071 ]\n",
      "  [ 0.3193225  -0.7948636  -0.08049449]]\n",
      "\n",
      " [[ 0.31998527 -0.72940385 -0.24933934]\n",
      "  [ 0.36260542 -0.7269818  -0.12308063]\n",
      "  [ 1.1260487  -0.79921776 -0.12928046]\n",
      "  ...\n",
      "  [ 0.28286287 -0.66268504 -0.3452552 ]\n",
      "  [ 0.21676128 -0.75599945 -0.91101426]\n",
      "  [ 0.32906157 -0.7931753  -0.07387184]]\n",
      "\n",
      " [[ 0.32837754 -0.72676593 -0.24656978]\n",
      "  [ 0.37321013 -0.72418267 -0.12261894]\n",
      "  [ 1.133832   -0.79683477 -0.13144553]\n",
      "  ...\n",
      "  [ 0.29173252 -0.659986   -0.34068933]\n",
      "  [ 0.22817676 -0.7544348  -0.9006868 ]\n",
      "  [ 0.33869058 -0.79131764 -0.06705078]]]\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the saved model weights\n",
    "checkpoint = torch.load('/home/maleen/research_data/Transformers/models/TF_tokenised/24_06_12_v1_best_model.pth', map_location=device)\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_joints = 14\n",
    "dropout_rate = 0.1\n",
    "input_length = 30\n",
    "predict_length = 30\n",
    "autoregressiveloops=30\n",
    "batch_size = 1\n",
    "dof=3\n",
    "input_dim = num_joints * dof\n",
    "\n",
    "# Initialize the models with the same configuration as during training\n",
    "embedding = SkeletalInputEmbedding(input_dim).to(device)\n",
    "#t_embedding = TargetEmbedding(num_joints=num_joints, dof=3, embed_dim=embed_dim,device=device).to(device)\n",
    "encoder = TransformerEncoder(embed_dim, num_heads, num_layers, dropout_rate).to(device)\n",
    "decoder = TransformerDecoder(embed_dim, num_heads, num_layers, num_joints, dropout_rate).to(device)\n",
    "\n",
    "# Load state dicts\n",
    "embedding.load_state_dict(checkpoint['embedding_state_dict'])\n",
    "#t_embedding.load_state_dict(checkpoint['t_embedding_state_dict'])\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "embedding.eval()\n",
    "#t_embedding.eval()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "X_pos, X_vel, X_acc, Y_pos, Y_vel, Y_acc = generate_sequences(norm_pos, norm_vel, norm_acc, input_length, predict_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_pos_tensor = torch.tensor(X_pos, dtype=torch.float32)\n",
    "X_vel_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "X_acc_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "\n",
    "\n",
    "Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "Y_vel_tensor = torch.tensor(Y_vel, dtype=torch.float32)\n",
    "Y_acc_tensor = torch.tensor(Y_acc, dtype=torch.float32)\n",
    "\n",
    "# Create the DataLoader for inference data\n",
    "dataset = TensorDataset(X_pos_tensor, X_vel_tensor, X_acc_tensor, Y_pos_tensor)\n",
    "inference_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prepare for autoregressive decoding\n",
    "predicted_positions = []\n",
    "\n",
    "criterion = MaskedMSELoss()\n",
    "\n",
    "# Perform inference across all batches\n",
    "for batch in inference_loader:\n",
    "    X_pos_batch, X_vel_batch, X_acc_batch, Y_pos_batch = [b.to(device) for b in batch]\n",
    "\n",
    "    # Encoder pass\n",
    "   \n",
    "    inputembeddings = embedding(X_pos_batch)\n",
    "    memory = encoder(inputembeddings, src_key_padding_mask=None)\n",
    "\n",
    "    \n",
    "    # Initialize the start token for decoding\n",
    "    current_pos = X_pos_batch[:, -1:, :, :]\n",
    "\n",
    "\n",
    "    for i in range(autoregressiveloops):\n",
    "        # Embed the current position\n",
    "        Y_expected= Y_pos_batch[:,i:i+1,:,:]\n",
    "    \n",
    "\n",
    "        # # #Running whole model\n",
    "        # if i > 0:\n",
    "        #     X_mask_batch_ar = torch.cat([X_mask_batch[:, 1:, :], current_mask], dim=1)\n",
    "        #     X_pos_batch_ar = torch.cat([X_pos_batch[:, 1:, :, :], current_pos], dim=1)\n",
    "    \n",
    "        #     src_key_padding_mask = ~X_mask_batch_ar.view(batch_size, input_length * num_joints)\n",
    "        #     input_embeddings = embedding(X_pos_batch_ar, X_mask_batch_ar)\n",
    "        #     memory = encoder(input_embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "        # ##\n",
    "        \n",
    "        current_embeddings = embedding(current_pos)\n",
    "        \n",
    "        # Decoder pass\n",
    "        output = decoder(current_embeddings, memory, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "        print(output.shape)\n",
    "    \n",
    "        # Update current_pos for the next prediction\n",
    "        current_pos = output[:, :, :, :].detach()  # only take the last timestep\n",
    "    \n",
    "        predicted_positions.append(current_pos.squeeze().cpu().numpy())\n",
    "    \n",
    "        output = output.where(~torch.isnan(output), torch.zeros_like(output))\n",
    "        # masked_output = output * Xmask\n",
    "    \n",
    "        Y_expected = Y_expected.where(~torch.isnan(Y_expected), torch.zeros_like(Y_expected))\n",
    "        # masked_y_pos = Y_pos_batch * Ymask\n",
    "\n",
    "        #print(Y_expected)\n",
    "    \n",
    "        # Compute loss\n",
    "        \n",
    "        loss = criterion(output, Y_expected)\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "    break\n",
    "# Convert the list of predicted positions to a more manageable form, e.g., a NumPy array\n",
    "predicted_positions = np.array(predicted_positions)\n",
    "\n",
    "print(\"Predicted Positions:\", predicted_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_normalization(normalized_data, medians_per_joint_axis, iqrs_per_joint_axis):\n",
    "    original_data = np.empty_like(normalized_data)  # Initialize an array to hold the original data\n",
    "\n",
    "    # Iterate over each joint and each axis\n",
    "    for joint in range(normalized_data.shape[0]):\n",
    "        for axis in range(normalized_data.shape[1]):\n",
    "            # Retrieve the median and IQR for this joint and axis\n",
    "            median = medians_per_joint_axis[joint, axis]\n",
    "            iqr = iqrs_per_joint_axis[joint, axis]\n",
    "\n",
    "            # Retrieve the normalized values for this joint and axis\n",
    "            normalized_values = normalized_data[joint, axis]\n",
    "\n",
    "            # Calculate the original values based on the normalization formula\n",
    "            original_values = (normalized_values * iqr) + median\n",
    "\n",
    "            # Store the original values in the output array\n",
    "            original_data[joint, axis] = original_values\n",
    "\n",
    "    return original_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data after reverse_normalization: (14, 3)\n",
      "Shape of data_y after reverse_normalization: (14, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1f04c2c86d49d2aac4cf71f7351333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Container(figure=Figure(box_center=[0.5, 0.5, 0.5], box_size=[1.0, 1.0, 1.0], camera=PerspectiveCamera(fov=45."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ipyvolume as ipv\n",
    "\n",
    "# Updated connections after removing joints 9, 10, 12, and 13\n",
    "updated_connections = [\n",
    "    (12, 10), (10, 0),     # Right Head\n",
    "    (13, 11), (11, 0),     # Left Head\n",
    "    (0, 1),                # Neck\n",
    "    (1, 2), (2, 3), (3, 4),          # Right arm\n",
    "    (1, 5), (5, 6), (6, 7),          # Left arm\n",
    "    (1, 8),                          # Right leg (up to hip)\n",
    "    (1, 9),                          # Left leg (up to hip)\n",
    "    (8, 9)                           # Between hips\n",
    "]\n",
    "\n",
    "pred = 19\n",
    "\n",
    "# Function to reverse normalization (assuming this is already defined)\n",
    "# def reverse_normalization(data, medians, iqrs): ...\n",
    "\n",
    "# First dataset processing (already provided code)\n",
    "data = reverse_normalization(predicted_positions[pred], medians_pos, iqrs_pos)\n",
    "print(\"Shape of data after reverse_normalization:\", data.shape)\n",
    "\n",
    "if len(data.shape) == 1:\n",
    "    print(\"Data is a scalar or has unexpected shape.\")\n",
    "else:\n",
    "    valid_keypoints = ~np.isnan(data[:, :3]).any(axis=1)\n",
    "    filtered_data = data[valid_keypoints]\n",
    "\n",
    "    # Create mapping from old indices to new indices after NaN removal\n",
    "    index_mapping = {old_index: new_index for new_index, old_index in enumerate(np.where(valid_keypoints)[0])}\n",
    "    # Create new connections for the first dataset\n",
    "    new_connections = [(index_mapping[start], index_mapping[end])\n",
    "                       for start, end in updated_connections\n",
    "                       if start in index_mapping and end in index_mapping]\n",
    "\n",
    "    # Second dataset processing\n",
    "    data_y = reverse_normalization(Y_pos[0][pred], medians_pos, iqrs_pos)\n",
    "    print(\"Shape of data_y after reverse_normalization:\", data_y.shape)\n",
    "\n",
    "    if len(data_y.shape) == 1:\n",
    "        print(\"Data_y is a scalar or has unexpected shape.\")\n",
    "    else:\n",
    "        valid_keypoints_y = ~np.isnan(data_y[:, :3]).any(axis=1)\n",
    "        filtered_data_y = data_y[valid_keypoints_y]\n",
    "\n",
    "        # Create mapping from old indices to new indices for the second dataset\n",
    "        index_mapping_y = {old_index: new_index for new_index, old_index in enumerate(np.where(valid_keypoints_y)[0])}\n",
    "        # Create new connections for the second dataset\n",
    "        new_connections_y = [(index_mapping_y[start], index_mapping_y[end])\n",
    "                             for start, end in updated_connections\n",
    "                             if start in index_mapping_y and end in index_mapping_y]\n",
    "\n",
    "        # Plot configuration\n",
    "        ipv.figure()\n",
    "\n",
    "        # Plot first dataset\n",
    "        scatter = ipv.scatter(filtered_data[:, 0], filtered_data[:, 1], filtered_data[:, 2], color='blue', marker='sphere', size=2)\n",
    "        for start, end in new_connections:\n",
    "            ipv.plot(filtered_data[[start, end], 0], filtered_data[[start, end], 1], filtered_data[[start, end], 2], color='red')\n",
    "\n",
    "        # Plot second dataset\n",
    "        scatter_y = ipv.scatter(filtered_data_y[:, 0], filtered_data_y[:, 1], filtered_data_y[:, 2], color='green', marker='sphere', size=2)\n",
    "        for start, end in new_connections_y:\n",
    "            ipv.plot(filtered_data_y[[start, end], 0], filtered_data_y[[start, end], 1], filtered_data_y[[start, end], 2], color='lime')\n",
    "\n",
    "        ipv.view(azimuth=0, elevation=-90)\n",
    "        ipv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 14, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False,  True, False, False,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

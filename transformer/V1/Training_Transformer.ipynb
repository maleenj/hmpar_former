{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileroot='2024_03_08_TRF_Maleen/'\n",
    "filename='2024_03_08_TRF_Maleen_01'\n",
    "\n",
    "datapath='/home/maleen/rosbags/Transformers/datasets/training/'\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "df1 = pd.read_pickle(datapath + filename + '.pkl')\n",
    "\n",
    "arrays = [np.array(item) for item in df1['Skeleton_3D']]\n",
    "timestamps = [np.array(item) for item in df1['Skeleton_Timestamp']]\n",
    "\n",
    "# Convert datetime to seconds from start\n",
    "timestamps = (timestamps - timestamps[0])\n",
    "\n",
    "# Stack these arrays along a new axis to create a 3D NumPy array\n",
    "# Each \"slice\" of this 3D array represents one frame of keypoints\n",
    "skeleton_3d_frames = np.stack(arrays, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create masks for the data (1 = data present, 0 = data missing)\n",
    "# Correcting the mask values\n",
    "masks = np.where(np.isnan(skeleton_3d_frames).any(axis=2), 0, 1)  # 0 for missing, 1 for present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the differences in position and time\n",
    "position_diff = np.diff(skeleton_3d_frames, axis=0)\n",
    "time_diff = np.diff(timestamps)\n",
    "\n",
    "# Ensure that time_diff is of shape (n,1,1) so that it broadcasts correctly when dividing\n",
    "time_diff = time_diff[:, np.newaxis, np.newaxis]\n",
    "\n",
    "# Update masks to match the dimensions of position_diff and time_diff\n",
    "# We use the bitwise AND operator to ensure that both the current and previous frames are valid\n",
    "masks_pos = masks[:-1, :] & masks[1:, :]\n",
    "\n",
    "# Add an additional dimension to masks\n",
    "masks_pos = masks_pos[:,:,np.newaxis]\n",
    "\n",
    "# Now we calculate velocity, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_vel = np.where(masks_pos, position_diff / time_diff, np.nan)\n",
    "\n",
    "masks_velocity = masks_pos[:-1, :] & masks_pos[1:, :]\n",
    "\n",
    "# Calculate the differences in velocity\n",
    "velocity_diff = np.diff(skel_vel, axis=0)\n",
    "\n",
    "# Now we calculate acceleration, handling missing data according to the mask\n",
    "# Where the mask is False, we will get np.nan\n",
    "skel_acc = np.where(masks_velocity, velocity_diff / time_diff[:-1, :, :], np.nan)  # Use time_diff with one less time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, slice skeleton_3d_frames and skel_vel to match the dimensions of skell_acc\n",
    "skel_pos= skeleton_3d_frames[2:]\n",
    "skel_vel = skel_vel[1:]\n",
    "masks = masks[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to hold the normalized data, medians, and IQRs\n",
    "norm_pos = np.empty_like(skel_pos)\n",
    "medians_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "iqrs_per_joint_axis_skel_pos = np.empty((skel_pos.shape[1], skel_pos.shape[2]))\n",
    "\n",
    "norm_vel= np.empty_like(skel_vel)\n",
    "medians_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "iqrs_per_joint_axis_vel = np.empty((skel_vel.shape[1], skel_vel.shape[2]))\n",
    "\n",
    "norm_acc = np.empty_like(skel_acc)\n",
    "medians_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "iqrs_per_joint_axis_acc = np.empty((skel_acc.shape[1], skel_acc.shape[2]))\n",
    "\n",
    "def robust_normalize_data_with_clipping(data, masks, medians_per_joint_axis, iqrs_per_joint_axis, normalized_data, clipping_percentiles=(1, 99)):\n",
    "    for joint in range(data.shape[1]):  # For each joint\n",
    "        for axis in range(data.shape[2]):  # For each axis (x, y, z)\n",
    "            joint_axis_data = data[:, joint, axis]\n",
    "            mask_for_joint = masks[:, joint]\n",
    "\n",
    "            # Select valid data based on the mask\n",
    "            valid_data = joint_axis_data[mask_for_joint == 1]\n",
    "\n",
    "            # Determine clipping thresholds based on percentiles\n",
    "            lower_threshold, upper_threshold = np.percentile(valid_data, clipping_percentiles) if valid_data.size > 0 else (np.nan, np.nan)\n",
    "\n",
    "            # Clip the data based on valid mask and thresholds\n",
    "            clipped_values = np.clip(joint_axis_data, lower_threshold, upper_threshold)\n",
    "\n",
    "            # Calculate median and IQR for clipped data\n",
    "            median = np.median(clipped_values[mask_for_joint == 1]) if np.any(mask_for_joint == 1) else np.nan\n",
    "            q75, q25 = np.percentile(clipped_values[mask_for_joint == 1], [75 ,25]) if np.any(mask_for_joint == 1) else (np.nan, np.nan)\n",
    "            iqr = q75 - q25\n",
    "\n",
    "            # Store the calculated medians and IQRs\n",
    "            medians_per_joint_axis[joint, axis] = median\n",
    "            iqrs_per_joint_axis[joint, axis] = iqr\n",
    "\n",
    "            # Normalize the clipped data, avoiding division by zero\n",
    "            if iqr > 0:\n",
    "                normalized_values = (clipped_values - median) / iqr\n",
    "            else:\n",
    "                normalized_values = clipped_values  # Keep original values if IQR is 0 or nan\n",
    "\n",
    "            # Apply normalization only where data is present\n",
    "            normalized_data[:, joint, axis] = np.where(mask_for_joint == 1, normalized_values, np.nan)\n",
    "            \n",
    "    return normalized_data, medians_per_joint_axis, iqrs_per_joint_axis\n",
    "\n",
    "# Example usage with your data\n",
    "# Note: masks, skel_pos, skel_vel, skel_acc should be defined in your context\n",
    "\n",
    "norm_pos, medians_pos, iqrs_pos = robust_normalize_data_with_clipping(skel_pos, masks, medians_per_joint_axis_skel_pos, iqrs_per_joint_axis_skel_pos, norm_pos)\n",
    "norm_vel, medians_vel, iqrs_vel = robust_normalize_data_with_clipping(skel_vel, masks, medians_per_joint_axis_vel, iqrs_per_joint_axis_vel, norm_vel)\n",
    "norm_acc, medians_acc, iqrs_acc = robust_normalize_data_with_clipping(skel_acc, masks, medians_per_joint_axis_acc, iqrs_per_joint_axis_acc, norm_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=skel_acc\n",
    "norm_data=norm_acc\n",
    "# Plot the original and normalized data for a specific joint and axis\n",
    "joint, axis = 0, 0  # Change as needed\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[:, joint, axis], bins=20, alpha=0.7, label='Original')\n",
    "plt.title(\"Original Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(norm_data[:, joint, axis], bins=20, alpha=0.7, label='Normalized')\n",
    "plt.title(\"Normalized Data Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check the median and range of the normalized data\n",
    "normalized_median = np.nanmedian(norm_data[:, joint, axis])\n",
    "print(\"Median of normalized data:\", normalized_median)\n",
    "\n",
    "within_iqr = ((norm_data[:, joint, axis] > -2) & (norm_data[:, joint, axis] < 2)).sum()\n",
    "print(f\"Data points within [-1, 1] (IQR): {within_iqr} out of {norm_data.shape[0]}\")\n",
    "\n",
    "within_iqr2 = ((data[:, joint, axis] > -1) & (data[:, joint, axis] < 1)).sum()\n",
    "print(f\"Data points within [-1, 1] (IQR): {within_iqr2} out of {norm_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletalInputEmbedding(nn.Module):\n",
    "    def __init__(self, num_joints=18, dof=3, embed_dim=128, seq_len=60):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.dof = dof\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.joint_embed = nn.Linear(dof, embed_dim)\n",
    "        self.vel_embed = nn.Linear(dof, embed_dim)\n",
    "        self.acc_embed = nn.Linear(dof, embed_dim)\n",
    "\n",
    "        self.register_buffer('positional_encoding', self.get_sinusoidal_encoding(seq_len * num_joints, embed_dim))\n",
    "\n",
    "    def forward(self, joint_positions, velocities, accelerations, mask=None):\n",
    "        joint_positions = joint_positions.view(-1, self.seq_len * self.num_joints, self.dof)\n",
    "        velocities = velocities.view(-1, self.seq_len * self.num_joints, self.dof)\n",
    "        accelerations = accelerations.view(-1, self.seq_len * self.num_joints, self.dof)\n",
    "\n",
    "        if mask is not None:\n",
    "            joint_positions = joint_positions * mask.view(-1, self.seq_len * self.num_joints, 1)\n",
    "            velocities = velocities * mask.view(-1, self.seq_len * self.num_joints, 1)\n",
    "            accelerations = accelerations * mask.view(-1, self.seq_len * self.num_joints, 1)\n",
    "\n",
    "        joint_embeddings = self.joint_embed(joint_positions)\n",
    "        vel_embeddings = self.vel_embed(velocities)\n",
    "        acc_embeddings = self.acc_embed(accelerations)\n",
    "\n",
    "        combined_embeddings = joint_embeddings + vel_embeddings + acc_embeddings\n",
    "        combined_embeddings += self.positional_encoding[:combined_embeddings.size(1), :].unsqueeze(0)\n",
    "        combined_embeddings = combined_embeddings.view(-1, self.seq_len, self.num_joints, self.embed_dim)\n",
    "\n",
    "        return combined_embeddings\n",
    "\n",
    "    def get_sinusoidal_encoding(self, seq_len, embed_dim):\n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # Reshape the input tensor to shape (seq_len, batch_size * num_joints, embed_dim)\n",
    "        batch_size, seq_len, num_joints, _ = x.shape\n",
    "        x = x.reshape(seq_len, batch_size * num_joints, self.embed_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Permute the mask tensor to shape (batch_size, seq_len, num_joints)\n",
    "            mask = mask.permute(0, 1, 2)\n",
    "\n",
    "            # Reshape the mask tensor to shape (batch_size * num_joints, seq_len)\n",
    "            mask = mask.reshape(batch_size * num_joints, seq_len)\n",
    "\n",
    "            # Create the padding mask\n",
    "            padding_mask = mask.eq(0)\n",
    "\n",
    "            # Pass the input and padding mask to the transformer encoder\n",
    "            x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        else:\n",
    "            x = self.transformer_encoder(x)\n",
    "\n",
    "        # Reshape the output tensor back to shape (batch_size, seq_len, num_joints, embed_dim)\n",
    "        x = x.reshape(batch_size, seq_len, num_joints, self.embed_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, dim_feedforward, num_joints, dof, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.num_joints = num_joints\n",
    "        self.dof = dof\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, dof)\n",
    "\n",
    "    def forward(self, tgt, memory, num_frames_to_predict, tgt_mask=None, memory_mask=None):\n",
    "        batch_size, seq_len, num_joints, _ = tgt.shape\n",
    "\n",
    "        # Reshape the target tensor to shape (seq_len, batch_size * num_joints, embed_dim)\n",
    "        tgt = tgt.reshape(seq_len, batch_size * num_joints, self.embed_dim)\n",
    "\n",
    "        # Reshape the memory tensor to shape (seq_len, batch_size * num_joints, embed_dim)\n",
    "        memory = memory.reshape(seq_len, batch_size * num_joints, self.embed_dim)\n",
    "\n",
    "        if tgt_mask is not None:\n",
    "            # Reshape the target mask tensor to shape (batch_size * num_joints, seq_len)\n",
    "            tgt_mask = tgt_mask.reshape(batch_size * num_joints, seq_len)\n",
    "            # Create the target padding mask\n",
    "            tgt_padding_mask = tgt_mask.eq(0)\n",
    "        else:\n",
    "            tgt_padding_mask = None\n",
    "\n",
    "        if memory_mask is not None:\n",
    "            # Reshape the memory mask tensor to shape (batch_size * num_joints, seq_len)\n",
    "            memory_mask = memory_mask.reshape(batch_size * num_joints, seq_len)\n",
    "            # Create the memory padding mask\n",
    "            memory_padding_mask = memory_mask.eq(0)\n",
    "        else:\n",
    "            memory_padding_mask = None\n",
    "\n",
    "        # Pass the target, memory, and masks to the transformer decoder\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                          memory_key_padding_mask=memory_padding_mask)\n",
    "\n",
    "        # Apply a linear transformation to the output\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # Reshape the output tensor to shape (batch_size, seq_len, num_joints, dof)\n",
    "        output = output.reshape(batch_size, seq_len, num_joints, self.dof)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sequences(norm_pos, norm_vel, norm_acc, mask, input_length=60, predict_length=60):\n",
    "    num_frames = norm_pos.shape[0]\n",
    "    num_joints = norm_pos.shape[1]\n",
    "    \n",
    "    # Calculate the total number of sequences we can create\n",
    "    num_sequences = num_frames - input_length - predict_length + 1\n",
    "\n",
    "    # Initialize arrays to store the input and target sequences\n",
    "    X_pos = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_vel = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    X_acc = np.zeros((num_sequences, input_length, num_joints, 3))\n",
    "    Y_pos = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_vel = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    Y_acc = np.zeros((num_sequences, predict_length, num_joints, 3))\n",
    "    X_mask = np.zeros((num_sequences, input_length, num_joints))\n",
    "    Y_mask = np.zeros((num_sequences, predict_length, num_joints))\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(num_sequences):\n",
    "        X_pos[i] = norm_pos[i:i+input_length]\n",
    "        X_vel[i] = norm_vel[i:i+input_length]\n",
    "        X_acc[i] = norm_acc[i:i+input_length]\n",
    "        Y_pos[i] = norm_pos[i+1:i+1+predict_length]\n",
    "        Y_vel[i] = norm_vel[i+1:i+1+predict_length]\n",
    "        Y_acc[i] = norm_acc[i+1:i+1+predict_length]\n",
    "        X_mask[i] = mask[i:i+input_length]\n",
    "        Y_mask[i] = mask[i+1:i+1+predict_length]\n",
    "\n",
    "    return X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask\n",
    "\n",
    "# Use the function\n",
    "X_pos, X_vel, X_acc, X_mask, Y_pos, Y_vel, Y_acc, Y_mask = generate_sequences(norm_pos, norm_vel, norm_acc, masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hook(module, input, output):\n",
    "    if torch.isnan(output).any():\n",
    "        print(f\"NaN detected in output of {module.__class__.__name__}\")\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    if any(torch.isnan(g).any() for g in grad_input if g is not None):\n",
    "        print(f\"NaN detected in grad input of {module.__class__.__name__}\")\n",
    "    if any(torch.isnan(g).any() for g in grad_output if g is not None):\n",
    "        print(f\"NaN detected in grad output of {module.__class__.__name__}\")\n",
    "\n",
    "def register_hooks(model):\n",
    "    for name, module in model.named_modules():\n",
    "        module.register_forward_hook(forward_hook)\n",
    "        module.register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_joints=18, dof=3, embed_dim=128, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embedding = SkeletalInputEmbedding(num_joints=num_joints, dof=dof, embed_dim=embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_encoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.decoder = TransformerDecoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_decoder_layers, dim_feedforward=dim_feedforward, num_joints=num_joints, dof=dof, dropout=dropout)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, pos, vel, acc, mask):\n",
    "        embeddings = self.input_embedding(pos, vel, acc, mask)\n",
    "        # if torch.isnan(embeddings).any():\n",
    "        #     print(\"NaN detected in embeddings\")\n",
    "\n",
    "        encoder_output = self.encoder(embeddings)\n",
    "        # if torch.isnan(encoder_output).any():\n",
    "        #     print(\"NaN detected in encoder output\")\n",
    "\n",
    "        predicted_output = self.decoder(encoder_output, encoder_output, num_frames_to_predict=60)\n",
    "        # if torch.isnan(predicted_output).any():\n",
    "        #     print(predicted_output)\n",
    "        \n",
    "        return predicted_output\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_pos_tensor = torch.tensor(X_pos, dtype=torch.float32)\n",
    "X_vel_tensor = torch.tensor(X_vel, dtype=torch.float32)\n",
    "X_acc_tensor = torch.tensor(X_acc, dtype=torch.float32)\n",
    "X_mask_tensor = torch.tensor(X_mask, dtype=torch.bool)\n",
    "\n",
    "Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_pos_tensor, X_vel_tensor, X_acc_tensor, X_mask_tensor, Y_pos_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel()\n",
    "model.cuda()  # Assuming you're using GPU\n",
    "register_hooks(model)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_loss = float('inf')\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for pos, vel, acc, mask, y_pos in dataloader:\n",
    "        pos, vel, acc, mask = pos.cuda(), vel.cuda(), acc.cuda(), mask.cuda()\n",
    "        y_pos = y_pos.cuda()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(pos, vel, acc, mask)\n",
    "        #print(output)\n",
    "    \n",
    "        mask = mask.unsqueeze(-1).expand_as(output)\n",
    "        output = output.where(~torch.isnan(output), torch.zeros_like(output))\n",
    "        y_pos = y_pos.where(~torch.isnan(y_pos), torch.zeros_like(y_pos))\n",
    "        \n",
    "        masked_output = output * mask\n",
    "        masked_y_pos = y_pos * mask\n",
    "        \n",
    "        loss = criterion(masked_output, masked_y_pos)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if torch.isnan(param).any():\n",
    "            print(f\"Pre-save NaN found in {name}\")\n",
    "        else:\n",
    "            print(f\"{name} - max: {param.max()}, min: {param.min()}, mean: {param.mean()}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), 'best_model_weights.pth')  # Save the best model weights\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss} (new best, model saved)')\n",
    "    else:\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "    print(\"Weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load weights:\", e)\n",
    "\n",
    "\n",
    "# Check for NaNs or extreme values in weights\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"NaN found in {name}\")\n",
    "    if param.abs().max() > 1e6:  # Example threshold for \"extreme\" values\n",
    "        print(f\"Extreme values found in {name}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformer_encoder():\n",
    "    model = TransformerEncoder(embed_dim=128, num_heads=8, num_layers=3, dim_feedforward=512)\n",
    "    batch_size, seq_len, num_joints = 2, 10, 5\n",
    "    x = torch.randn(batch_size, seq_len, num_joints, 128)  # Simulate input\n",
    "    mask = torch.randint(0, 2, (batch_size, seq_len, num_joints), dtype=torch.bool)  # Simulate binary mask\n",
    "\n",
    "    output = model(x, mask)\n",
    "    assert output.shape == (batch_size, seq_len, num_joints, 128), \"Output shape mismatch\"\n",
    "\n",
    "test_transformer_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_3d_frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
